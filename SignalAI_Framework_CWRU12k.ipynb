{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPXmkt-iKCeA"
      },
      "source": [
        "# Install libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTHYLssWfNUu",
        "outputId": "46bb7f4e-522c-4898-b1bc-ecc7024b2b03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting vibdata==1.1.1\n",
            "  Downloading vibdata-1.1.1-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting signalAI==0.0.4\n",
            "  Downloading signalAI-0.0.4-py3-none-any.whl.metadata (446 bytes)\n",
            "Collecting essentia (from vibdata==1.1.1)\n",
            "  Downloading essentia-2.1b6.dev1389-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (from vibdata==1.1.1) (5.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from vibdata==1.1.1) (2.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from vibdata==1.1.1) (4.12.0.88)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from vibdata==1.1.1) (2.2.2)\n",
            "Collecting rarfile (from vibdata==1.1.1)\n",
            "  Downloading rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vibdata==1.1.1) (2.32.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from vibdata==1.1.1) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from vibdata==1.1.1) (1.16.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from vibdata==1.1.1) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from vibdata==1.1.1) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from signalAI==0.0.4) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from signalAI==0.0.4) (0.13.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from essentia->vibdata==1.1.1) (6.0.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from essentia->vibdata==1.1.1) (1.17.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown->vibdata==1.1.1) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown->vibdata==1.1.1) (3.20.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->signalAI==0.0.4) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->signalAI==0.0.4) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->signalAI==0.0.4) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->signalAI==0.0.4) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->signalAI==0.0.4) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->signalAI==0.0.4) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->signalAI==0.0.4) (3.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->signalAI==0.0.4) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->vibdata==1.1.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->vibdata==1.1.1) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vibdata==1.1.1) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vibdata==1.1.1) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vibdata==1.1.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vibdata==1.1.1) (2026.1.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->vibdata==1.1.1) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->vibdata==1.1.1) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->vibdata==1.1.1) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->vibdata==1.1.1) (1.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown->vibdata==1.1.1) (2.8.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->vibdata==1.1.1) (3.0.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown->vibdata==1.1.1) (1.7.1)\n",
            "Downloading vibdata-1.1.1-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading signalAI-0.0.4-py3-none-any.whl (16 kB)\n",
            "Downloading essentia-2.1b6.dev1389-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading rarfile-4.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: rarfile, essentia, vibdata, signalAI\n",
            "Successfully installed essentia-2.1b6.dev1389 rarfile-4.2 signalAI-0.0.4 vibdata-1.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install vibdata==1.1.1 signalAI==0.0.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGC5F4WuQ8Qs"
      },
      "source": [
        "# Import Libs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "6pod38v0RKKx",
        "outputId": "c37ec1db-237c-41f3-b7a1-1c31d3eb7326"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Basic imports\n",
        "import numpy as np\n",
        "import numpy.typing as npt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# vibdata\n",
        "import vibdata.raw as raw_datasets\n",
        "from vibdata.deep.DeepDataset import DeepDataset, convertDataset\n",
        "from vibdata.deep.signal.transforms import (\n",
        "    Sequential,\n",
        "    SplitSampleRate,\n",
        "    FeatureExtractor,\n",
        "    FilterByValue,\n",
        "    Split\n",
        ")\n",
        "from vibdata.deep.signal.core import SignalSample\n",
        "\n",
        "# SignalAI\n",
        "from signalAI.experiments.features_1d import Features1DExperiment\n",
        "from signalAI.utils.group_dataset import GroupDataset\n",
        "from signalAI.utils.fold_idx_generator import (\n",
        "    FoldIdxGeneratorUnbiased,\n",
        "    FoldIdxGeneratorBiased,\n",
        ")\n",
        "\n",
        "class GroupCWRULoad(GroupDataset):\n",
        "    @staticmethod\n",
        "    def _assigne_group(sample: SignalSample) -> int:\n",
        "        return sample[\"metainfo\"][\"load\"]\n",
        "\n",
        "class GroupCWRUSeverity(GroupDataset):\n",
        "    @staticmethod\n",
        "    def _assigne_group(sample: SignalSample) -> int:\n",
        "        severity = sample[\"metainfo\"][\"fault_size\"]\n",
        "\n",
        "        match severity:\n",
        "            case 0.0:\n",
        "                return sample[\"metainfo\"][\"load\"]\n",
        "            case 0.007:\n",
        "                return 0\n",
        "            case 0.014:\n",
        "                return 1\n",
        "            case 0.021:\n",
        "                return 2\n",
        "            case 0.028:\n",
        "                return 3\n",
        "\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gw9EnsSs276"
      },
      "source": [
        "# Deep Learning Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRt2UWNpaPbh"
      },
      "source": [
        "## Import CRWU dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cckVlJSzUqnN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cached downloading...\n",
            "Hash: md5:d7d3042161080fc82e99d78464fa2914\n",
            "From (original): https://drive.google.com/uc?id=1G2vfms1QDlkdzqL_LAQdMIQAoludxBNj\n",
            "From (redirected): https://drive.google.com/uc?id=1G2vfms1QDlkdzqL_LAQdMIQAoludxBNj&confirm=t&uuid=8a5ab875-285a-491a-b839-1798a9b4eaba\n",
            "To: ../data/raw_data/cwru/CWRU_raw/CWRU.zip\n",
            "100%|██████████| 245M/245M [00:02<00:00, 92.0MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "raw_root_dir = \"../data/raw_data/cwru\"\n",
        "raw_dataset = raw_datasets.CWRU_raw(raw_root_dir, download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rDx-9rht7Un"
      },
      "source": [
        "## Time domain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmIzuXr-aVBm"
      },
      "source": [
        "### Filter by 12k SampleRate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t09LPCnRtMbu",
        "outputId": "6aa32f9b-c9b5-4eb0-9507-1a6894c56c17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(transforms=[SplitSampleRate()])\n"
          ]
        }
      ],
      "source": [
        "transforms_time = Sequential(\n",
        "    [\n",
        "        SplitSampleRate()\n",
        "    ]\n",
        ")\n",
        "print(transforms_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NeeJ_8J3tRno"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformando\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting CWRU: 100%|██████████| 10/10 [00:01<00:00,  5.31it/s]\n"
          ]
        }
      ],
      "source": [
        "deep_root_dir_time = \"../data/deep_data/deep_learning\"\n",
        "deep_dataset_time = convertDataset(raw_dataset,filter=FilterByValue(on_field=\"sample_rate\", values=12000),transforms=transforms_time, dir_path=deep_root_dir_time, batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gufU833iax3I"
      },
      "source": [
        "## Generate Unbiased Folds Single Round"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd86Qz_Qth8u",
        "outputId": "74a72abc-7c83-4872-e1f3-7e698abb2344"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Grouping dataset: 100%|██████████| 2520/2520 [00:00<00:00, 8102.19sample/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 3., 3., 3.])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "folds_singleround_deep = FoldIdxGeneratorUnbiased(deep_dataset_time, GroupCWRULoad , dataset_name=\"CWRU12k_deep\").generate_folds()\n",
        "folds_singleround_deep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Unbiased Folds MultiRound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GroupMultiRoundCWRULoad(GroupDataset):\n",
        "    @staticmethod\n",
        "    def _assigne_group(sample: SignalSample) -> int:\n",
        "        sample_metainfo = sample[\"metainfo\"]\n",
        "        return sample_metainfo[\"label\"].astype(str) + \" \" + sample_metainfo[\"load\"].astype(int).astype(str)\n",
        "\n",
        "CLASS_DEF = {0: \"N\", 1: \"O\", 2: \"I\", 3: \"R\"}\n",
        "CONDITION_DEF = {\"0\": \"0\", \"1\": \"1\", \"2\": \"2\", \"3\": \"3\"}\n",
        "folds_multiround_deep = FoldIdxGeneratorUnbiased(deep_dataset,\n",
        "                                    GroupMultiRoundCWRULoad ,\n",
        "                                    dataset_name=\"CWRU\",\n",
        "                                    multiround=True,\n",
        "                                    class_def=CLASS_DEF,\n",
        "                                    condition_def=CONDITION_DEF).generate_folds()\n",
        "folds_multiround_deep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL1DdMvqe7Qs"
      },
      "source": [
        "## DeepLearning Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt4WUe1EkNUQ"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Sbq0v5_lgAAW"
      },
      "outputs": [],
      "source": [
        "# vibclassifier/experiments/base.py\n",
        "from abc import ABC, abstractmethod\n",
        "import json\n",
        "from typing import Optional, Dict, Any\n",
        "from vibdata.raw.base import RawVibrationDataset\n",
        "from vibdata.deep.signal.transforms import Transform\n",
        "\n",
        "class Experiment(ABC):\n",
        "    \"\"\"Classe base abstrata para todos os experimentos de classificação de vibração.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        description: str,\n",
        "        dataset: Optional[RawVibrationDataset] = None,\n",
        "        data_transform: Optional[Transform] = None,\n",
        "        feature_selector = None,\n",
        "        model = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Inicializa o experimento.\n",
        "\n",
        "        Args:\n",
        "            name: Nome identificador do experimento\n",
        "            description: Descrição detalhada do experimento\n",
        "            dataset: Conjunto de dados de vibração\n",
        "            data_transform: Transformação a ser aplicada nos dados brutos\n",
        "            data_division_method: Método de divisão dos dados (e.g., 'kfold', 'holdout')\n",
        "            data_division_params: Parâmetros para o método de divisão\n",
        "            feature_selector: Seletor de features (para experimentos com extração)\n",
        "            model: Modelo de machine learning/deep learning\n",
        "        \"\"\"\n",
        "        self.name = name\n",
        "        self.description = description\n",
        "        self.dataset = dataset\n",
        "        self.data_transform = data_transform\n",
        "        self.feature_selector = feature_selector\n",
        "        self.model = model\n",
        "\n",
        "        # Resultados serão armazenados aqui\n",
        "        self.results = {}\n",
        "\n",
        "    @abstractmethod\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Prepara os dados para o experimento.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def run(self):\n",
        "        \"\"\"Executa o experimento completo.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def save_results(self, filepath: str):\n",
        "        \"\"\"Salva os resultados do experimento.\"\"\"\n",
        "        # Implementação básica - pode ser extendida\n",
        "        with open(filepath, 'w') as f:\n",
        "            json.dump(self.results, f)\n",
        "\n",
        "    def load_results(self, filepath: str):\n",
        "        \"\"\"Carrega resultados de um experimento anterior.\"\"\"\n",
        "        with open(filepath, 'r') as f:\n",
        "            self.results = json.load(f)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Experiment: {self.name}\\nDescription: {self.description}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pS8nDnOpe_Le"
      },
      "outputs": [],
      "source": [
        "# vibclassifier/experiments/deep_torch.py\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from typing import List, Dict, Optional, Tuple, Union\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from signalAI.utils.metrics import calculate_metrics\n",
        "from signalAI.utils.experiment_result import ExperimentResults, FoldResults\n",
        "import copy\n",
        "\n",
        "class TorchVibrationDataset(Dataset):\n",
        "    \"\"\"Wrapper to convert dataset samples into Torch tensors.\"\"\"\n",
        "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Função auxiliar KL\n",
        "def kl_divergence(rho, rho_hat):\n",
        "    rho_hat = torch.mean(rho_hat, dim=0)\n",
        "    rho = torch.tensor([rho] * len(rho_hat), device=rho_hat.device)\n",
        "    epsilon = 1e-7\n",
        "    term1 = rho * torch.log((rho + epsilon) / (rho_hat + epsilon))\n",
        "    term2 = (1 - rho) * torch.log((1 - rho + epsilon) / (1 - rho_hat + epsilon))\n",
        "    return torch.sum(term1 + term2)\n",
        "\n",
        "class DeepLearningExperiment(Experiment):\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        description: str,\n",
        "        dataset,\n",
        "        data_fold_idxs: List[int],\n",
        "        model: nn.Module,\n",
        "        criterion: Optional[nn.Module] = None,\n",
        "        # Parâmetros adaptados para o autoencoder\n",
        "        reconstruction_criterion: Optional[nn.Module] = None,\n",
        "        recon_loss_weight: float = 1.0,\n",
        "        sparsity_target: Optional[float] = None,\n",
        "        sparsity_weight: float = 0.0,\n",
        "        pretrain_epochs: int = 0, # Épocas de treinamento do autoencoder\n",
        "        optimizer_class: Optional[torch.optim.Optimizer] = optim.Adam,\n",
        "        batch_size: int = 32,\n",
        "        lr: float = 1e-3,\n",
        "        num_epochs: int = 20, # Épocas de treino do classificador\n",
        "        val_split: float = 0.2,\n",
        "        output_dir: str = \"results_torch\",\n",
        "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(name, description, dataset, model=model, **kwargs)\n",
        "        self.data_fold_idxs = data_fold_idxs\n",
        "        self.output_dir = Path(output_dir)\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "        self.pretrain_epochs = pretrain_epochs\n",
        "        self.val_split = val_split\n",
        "        self.device = device\n",
        "        self.optimizer_class = optimizer_class\n",
        "        self.lr = lr\n",
        "        self.criterion = criterion if criterion is not None else nn.CrossEntropyLoss()\n",
        "        \n",
        "        self.reconstruction_criterion = reconstruction_criterion\n",
        "        self.recon_loss_weight = recon_loss_weight\n",
        "        self.sparsity_target = sparsity_target\n",
        "        self.sparsity_weight = sparsity_weight\n",
        "        \n",
        "        self.is_sae_task = self.sparsity_target is not None and self.sparsity_weight > 0.0\n",
        "        # Define tipo do AutoEncoder utilizado\n",
        "        self.is_autoencoder_task = reconstruction_criterion is not None or self.is_sae_task or \"AE1D\" in model.__class__.__name__\n",
        "\n",
        "        if self.is_sae_task and self.reconstruction_criterion is None:\n",
        "             print(\"Warning: SAE task detected but no reconstruction_criterion. Defaulting to MSELoss.\")\n",
        "             self.reconstruction_criterion = nn.MSELoss()\n",
        "\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
        "            model = torch.nn.DataParallel(model) \n",
        "        \n",
        "        # Guarda encoder e decoder\n",
        "        self.original_model = model.module if isinstance(model, nn.DataParallel) else model\n",
        "\n",
        "        self.n_outer_folds = len(np.unique(data_fold_idxs))\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.prepare_data()\n",
        "\n",
        "    def prepare_data(self):\n",
        "        features, labels = [], []\n",
        "        for sample in self.dataset:\n",
        "            features.append(sample['signal'][0]) \n",
        "            labels.append(sample['metainfo']['label'])\n",
        "        self.X = np.array(features)\n",
        "        self.y = np.array(labels)\n",
        "\n",
        "    def _train_one_fold(\n",
        "        self, X_train, y_train, X_test, y_test, fold_idx: int\n",
        "    ) -> FoldResults:\n",
        "        \n",
        "        train_dataset = TorchVibrationDataset(X_train, y_train)\n",
        "        test_dataset = TorchVibrationDataset(X_test, y_test)\n",
        "        val_size = int(self.val_split * len(train_dataset))\n",
        "        train_size = len(train_dataset) - val_size\n",
        "        train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
        "        \n",
        "        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "        model = copy.deepcopy(self.model.to(self.device))\n",
        "        model_core = model.module if isinstance(model, nn.DataParallel) else model\n",
        "\n",
        "        # Verificação da estrutura de AE (encoder, decoder, classifier)\n",
        "        has_ae_structure = hasattr(model_core, 'encoder') and hasattr(model_core, 'decoder') and hasattr(model_core, 'classifier')\n",
        "\n",
        "        # Treinamento do AutoEncoder\n",
        "        if self.is_autoencoder_task and self.pretrain_epochs > 0 and has_ae_structure:\n",
        "            print(f\"[Fold {fold_idx}] AutoEncoder training ({self.pretrain_epochs} epochs)...\")\n",
        "            \n",
        "            # Otimizador Encoder + Decoder\n",
        "            optimizer_ae = self.optimizer_class([\n",
        "                {'params': model_core.encoder.parameters()},\n",
        "                {'params': model_core.decoder.parameters()}\n",
        "            ], lr=self.lr)\n",
        "\n",
        "            for epoch in range(self.pretrain_epochs):\n",
        "                model.train()\n",
        "                running_recon_loss = 0.0\n",
        "                \n",
        "                for xb, _ in train_loader:\n",
        "                    xb = xb.to(self.device)\n",
        "                    input_data = xb\n",
        "\n",
        "                    if any(isinstance(m, nn.Conv1d) for m in model.modules()) and xb.ndim == 2:\n",
        "                        xb = xb.unsqueeze(1)\n",
        "                    elif any(isinstance(m, nn.Conv2d) for m in model.modules()) and xb.ndim == 2:\n",
        "                         side = int(np.sqrt(xb.shape[1])); xb = xb.view(xb.size(0), 1, side, side)\n",
        "\n",
        "                    optimizer_ae.zero_grad()\n",
        "                    outputs = model(xb) # (class, recon, [sparsity])\n",
        "\n",
        "                    if isinstance(outputs, tuple):\n",
        "                        # Foco na reconstrução\n",
        "                        reconstruction = outputs[1] \n",
        "                        \n",
        "                        loss = self.reconstruction_criterion(reconstruction, input_data)\n",
        "                        \n",
        "                        # Adiciona esparsidade se for SAE\n",
        "                        if self.is_sae_task and len(outputs) > 2:\n",
        "                            latent_features = outputs[2]\n",
        "                            loss += self.sparsity_weight * kl_divergence(self.sparsity_target, latent_features)\n",
        "                        \n",
        "                        loss.backward()\n",
        "                        optimizer_ae.step()\n",
        "                        running_recon_loss += loss.item() * input_data.size(0)\n",
        "                \n",
        "                avg_recon_loss = running_recon_loss / len(train_loader.dataset)\n",
        "                if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "                    print(f\"  [Pre-train] Epoch {epoch+1}/{self.pretrain_epochs} Recon Loss: {avg_recon_loss:.4f}\")\n",
        "\n",
        "        # Treino do classificador\n",
        "        print(f\"[Fold {fold_idx}] Classifier training ({self.num_epochs} epochs)...\")\n",
        "\n",
        "        # Define otimizador para a fase supervisionada\n",
        "        if has_ae_structure and self.is_autoencoder_task:\n",
        "            # Se for AE: Treina Encoder + Classifier (Decoder congelado ou ignorado pelo otimizador)\n",
        "            optimizer_clf = self.optimizer_class([\n",
        "                {'params': model_core.encoder.parameters()},\n",
        "                {'params': model_core.classifier.parameters()}\n",
        "            ], lr=self.lr)\n",
        "        else:\n",
        "            # Se for MLP/CNN padrão: Treina todos os parâmetros\n",
        "            optimizer_clf = self.optimizer_class(model.parameters(), lr=self.lr)\n",
        "\n",
        "        train_losses, val_losses = [], []\n",
        "        \n",
        "        for epoch in range(self.num_epochs):\n",
        "            epoch_start = time.time()\n",
        "            model.train()\n",
        "            running_loss = 0.0\n",
        "            \n",
        "            for xb, yb in train_loader:\n",
        "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
        "                \n",
        "                # Ajuste de shape\n",
        "                if any(isinstance(m, nn.Conv1d) for m in model.modules()) and xb.ndim == 2:\n",
        "                     xb = xb.unsqueeze(1)\n",
        "                elif any(isinstance(m, nn.Conv2d) for m in model.modules()) and xb.ndim == 2:\n",
        "                     side = int(np.sqrt(xb.shape[1])); xb = xb.view(xb.size(0), 1, side, side)\n",
        "\n",
        "                optimizer_clf.zero_grad()\n",
        "                outputs = model(xb)\n",
        "\n",
        "                # Cálculo da perda apenas de CLASSIFICAÇÃO\n",
        "                if isinstance(outputs, tuple):\n",
        "                    classification_output = outputs[0] # Pega apenas a classificação\n",
        "                else:\n",
        "                    classification_output = outputs # Modelo padrão\n",
        "\n",
        "                loss = self.criterion(classification_output, yb)\n",
        "                \n",
        "                loss.backward()\n",
        "                optimizer_clf.step()\n",
        "                running_loss += loss.item() * xb.size(0)\n",
        "\n",
        "            avg_train_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "            # Validação\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for xb, yb in val_loader:\n",
        "                    xb, yb = xb.to(self.device), yb.to(self.device)\n",
        "                    # Ajuste de shape\n",
        "                    if any(isinstance(m, nn.Conv1d) for m in model.modules()) and xb.ndim == 2:\n",
        "                         xb = xb.unsqueeze(1)\n",
        "                    elif any(isinstance(m, nn.Conv2d) for m in model.modules()) and xb.ndim == 2:\n",
        "                         side = int(np.sqrt(xb.shape[1])); xb = xb.view(xb.size(0), 1, side, side)\n",
        "\n",
        "                    outputs = model(xb)\n",
        "                    \n",
        "                    if isinstance(outputs, tuple):\n",
        "                        classification_output = outputs[0]\n",
        "                    else:\n",
        "                        classification_output = outputs\n",
        "\n",
        "                    loss = self.criterion(classification_output, yb)\n",
        "                    val_loss += loss.item() * xb.size(0)\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader.dataset)\n",
        "            \n",
        "            train_losses.append(avg_train_loss)\n",
        "            val_losses.append(avg_val_loss)\n",
        "            \n",
        "            epoch_time = time.time() - epoch_start\n",
        "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
        "                print(f\"  [Supervised] Epoch {epoch+1}/{self.num_epochs} Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Time: {epoch_time:.2f}s\")\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(train_losses, label=\"Train Loss (Clf)\")\n",
        "        plt.plot(val_losses, label=\"Val Loss (Clf)\")\n",
        "        plt.legend(); plt.title(f\"Loss Curve - Fold {fold_idx}\")\n",
        "        plt.savefig(os.path.join(self.dir_path, f\"loss_curve_fold{fold_idx}_{self.start_time}.png\")); plt.close()\n",
        "        \n",
        "        torch.save(model_core.state_dict(), os.path.join(self.dir_path, f\"model_fold{fold_idx}.pt\"))\n",
        "\n",
        "        # Teste\n",
        "        y_true, y_pred, y_proba = [], [], []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in test_loader:\n",
        "                xb, yb = xb.to(self.device), yb.to(self.device)\n",
        "                if any(isinstance(m, nn.Conv1d) for m in model.modules()) and xb.ndim == 2:\n",
        "                     xb = xb.unsqueeze(1)\n",
        "                elif any(isinstance(m, nn.Conv2d) for m in model.modules()) and xb.ndim == 2:\n",
        "                     side = int(np.sqrt(xb.shape[1])); xb = xb.view(xb.size(0), 1, side, side)\n",
        "\n",
        "                outputs = model(xb)\n",
        "                if isinstance(outputs, tuple):\n",
        "                    classification_output = outputs[0]\n",
        "                else:\n",
        "                    classification_output = outputs\n",
        "\n",
        "                probs = torch.softmax(classification_output, dim=1)\n",
        "                preds = torch.argmax(probs, dim=1)\n",
        "                y_true.extend(yb.cpu().numpy()); y_pred.extend(preds.cpu().numpy()); y_proba.extend(probs.cpu().numpy())\n",
        "\n",
        "        metrics = calculate_metrics(np.array(y_true), np.array(y_pred), np.array(y_proba))\n",
        "        return FoldResults(fold_idx, np.array(y_true), np.array(y_pred), np.array(y_proba), metrics)\n",
        "\n",
        "    def run(self) -> ExperimentResults:\n",
        "        self.start_time = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.dir_path = os.path.join(self.output_dir, f\"results_{self.name}_{self.start_time}\")\n",
        "        os.makedirs(self.dir_path, exist_ok=True)\n",
        "\n",
        "        results = ExperimentResults(\n",
        "            experiment_name=self.name, description=self.description,\n",
        "            model_name=self.original_model.__class__.__name__, feature_names=None,\n",
        "            config={'n_outer_folds': self.n_outer_folds, 'pretrain_epochs': self.pretrain_epochs, \n",
        "                    'finetune_epochs': self.num_epochs, 'batch_size': self.batch_size, 'lr': self.lr}\n",
        "        )\n",
        "\n",
        "        for outer_fold in range(self.n_outer_folds):\n",
        "            print(f\"\\n=== Outer Fold {outer_fold+1}/{self.n_outer_folds} ===\")\n",
        "            train_mask = self.data_fold_idxs != outer_fold\n",
        "            test_mask = self.data_fold_idxs == outer_fold\n",
        "            \n",
        "            try:\n",
        "                fold_result = self._train_one_fold(self.X[train_mask], self.y[train_mask], self.X[test_mask], self.y[test_mask], outer_fold)\n",
        "                results.add_fold_result(fold_result)\n",
        "                print(f\"  Result: Acc={fold_result.metrics['accuracy']:.4f}, F1={fold_result.metrics['f1']:.4f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error in fold {outer_fold}: {e}\")\n",
        "                import traceback; traceback.print_exc()\n",
        "\n",
        "        results.calculate_overall_metrics()\n",
        "        results.save_json(os.path.join(self.dir_path, f\"results.json\"))\n",
        "        print(\"\\n=== Final Results ===\")\n",
        "        print(f\"Mean Accuracy: {results.overall_metrics['accuracy']:.4f}\")\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1D MLP adaptado para 12k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP1D_12k(nn.Module):\n",
        "    def __init__(self, input_length: int = 12000, num_classes: int = 4):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Camada adicional para adaptar a entrada de 12000 para 1024 progressivamente\n",
        "        self.adaptation_layers = nn.Sequential(\n",
        "            # 12000 -> 4096\n",
        "            nn.Linear(input_length, 4096),\n",
        "            nn.BatchNorm1d(4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.7), # Dropout alto para evitar overfitting na entrada\n",
        "\n",
        "            # 4096 -> 2048\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "\n",
        "            # 2048 -> 1024 (Conecta com a arquitetura original)\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.4)\n",
        "        )\n",
        "\n",
        "        # Arquitetura original:        \n",
        "        self.fc3 = nn.Sequential(\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.fc4 = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.fc5 = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.fc6 = nn.Sequential(\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.fc7 = nn.Sequential(\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.flatten(x, 1)\n",
        "        \n",
        "        # Passa pela adaptação primeiro\n",
        "        out = self.adaptation_layers(out)\n",
        "        \n",
        "        # Segue o fluxo normal\n",
        "        out = self.fc3(out)\n",
        "        out = self.fc4(out)\n",
        "        out = self.fc5(out)\n",
        "        out = self.fc6(out)\n",
        "        out = self.fc7(out)\n",
        "        \n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input length: 12000, Num classes: 4\n",
            "\n",
            "=== Outer Fold 1/4 ===\n",
            "[Fold 0] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.4053, Val Loss: 1.4620, Time: 0.67s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 0.9788, Val Loss: 1.4911, Time: 0.63s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.3154, Val Loss: 1.2524, Time: 0.64s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.2065, Val Loss: 1.2888, Time: 0.63s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.1697, Val Loss: 1.3204, Time: 0.64s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.1517, Val Loss: 1.3577, Time: 0.65s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.1044, Val Loss: 1.4491, Time: 0.64s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.1078, Val Loss: 1.2969, Time: 0.63s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.0846, Val Loss: 1.3382, Time: 0.63s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.0816, Val Loss: 1.3492, Time: 0.65s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.0654, Val Loss: 1.4990, Time: 0.63s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.0720, Val Loss: 1.3685, Time: 0.63s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.0809, Val Loss: 1.4297, Time: 0.63s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.0736, Val Loss: 1.5588, Time: 0.65s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.0541, Val Loss: 1.5312, Time: 0.63s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.0662, Val Loss: 1.6912, Time: 0.63s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.0748, Val Loss: 1.4908, Time: 0.63s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.0340, Val Loss: 1.5351, Time: 0.63s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.0335, Val Loss: 1.5404, Time: 0.63s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.0442, Val Loss: 1.5242, Time: 0.63s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.0394, Val Loss: 1.5609, Time: 0.65s\n",
            "  Result: Acc=0.3767, F1=0.3657\n",
            "\n",
            "=== Outer Fold 2/4 ===\n",
            "[Fold 1] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.4063, Val Loss: 1.4640, Time: 0.65s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 0.9661, Val Loss: 1.5965, Time: 0.63s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.3395, Val Loss: 1.5464, Time: 0.64s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.2185, Val Loss: 1.7093, Time: 0.63s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.1978, Val Loss: 1.6954, Time: 0.63s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.1457, Val Loss: 2.0532, Time: 0.64s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.1086, Val Loss: 1.8747, Time: 0.63s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.1325, Val Loss: 1.9441, Time: 0.65s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.0688, Val Loss: 2.0359, Time: 0.63s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.0872, Val Loss: 1.8756, Time: 0.64s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.0756, Val Loss: 1.8989, Time: 0.64s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.0836, Val Loss: 1.9070, Time: 0.66s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.0595, Val Loss: 1.9928, Time: 0.63s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.0576, Val Loss: 2.2675, Time: 0.64s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.0613, Val Loss: 2.0787, Time: 0.64s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.0521, Val Loss: 2.1239, Time: 0.65s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.0467, Val Loss: 2.0809, Time: 0.64s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.0688, Val Loss: 2.0655, Time: 0.64s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.0497, Val Loss: 2.2584, Time: 0.64s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.0475, Val Loss: 2.3412, Time: 0.64s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.0734, Val Loss: 2.0428, Time: 0.64s\n",
            "  Result: Acc=0.4422, F1=0.4165\n",
            "\n",
            "=== Outer Fold 3/4 ===\n",
            "[Fold 2] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.4075, Val Loss: 1.4442, Time: 0.64s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 0.9480, Val Loss: 1.5668, Time: 0.64s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.3708, Val Loss: 1.5568, Time: 0.68s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.2230, Val Loss: 1.7234, Time: 0.63s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.1382, Val Loss: 1.6441, Time: 0.64s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.1501, Val Loss: 1.7704, Time: 0.64s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.1004, Val Loss: 1.6275, Time: 0.64s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.0759, Val Loss: 1.6510, Time: 0.63s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.1310, Val Loss: 1.7497, Time: 0.63s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.1000, Val Loss: 1.6904, Time: 0.65s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.1059, Val Loss: 1.7929, Time: 0.64s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.0735, Val Loss: 1.8296, Time: 0.63s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.0418, Val Loss: 1.8530, Time: 0.63s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.0449, Val Loss: 1.9116, Time: 0.65s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.0933, Val Loss: 2.0366, Time: 0.64s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.0575, Val Loss: 2.0542, Time: 0.63s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.0524, Val Loss: 2.0952, Time: 0.64s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.0476, Val Loss: 1.8342, Time: 0.64s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.0517, Val Loss: 1.8915, Time: 0.64s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.0504, Val Loss: 1.9619, Time: 0.64s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.0434, Val Loss: 2.0765, Time: 0.65s\n",
            "  Result: Acc=0.4875, F1=0.4743\n",
            "\n",
            "=== Outer Fold 4/4 ===\n",
            "[Fold 3] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.4141, Val Loss: 1.5047, Time: 0.67s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 0.9810, Val Loss: 1.7489, Time: 0.64s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.3757, Val Loss: 1.6542, Time: 0.63s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.2421, Val Loss: 1.6616, Time: 0.63s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.1600, Val Loss: 1.8284, Time: 0.65s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.1522, Val Loss: 1.6740, Time: 0.64s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.1132, Val Loss: 1.8794, Time: 0.64s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.0784, Val Loss: 1.7818, Time: 0.64s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.1179, Val Loss: 1.9443, Time: 0.63s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.0737, Val Loss: 1.8359, Time: 0.63s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.0778, Val Loss: 2.0557, Time: 0.63s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.0535, Val Loss: 2.0249, Time: 0.64s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.0588, Val Loss: 1.9867, Time: 0.63s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.0565, Val Loss: 2.2240, Time: 0.63s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.0561, Val Loss: 1.9953, Time: 0.63s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.0499, Val Loss: 2.0354, Time: 0.67s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.0449, Val Loss: 2.0043, Time: 0.64s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.0423, Val Loss: 1.8673, Time: 0.64s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.0471, Val Loss: 2.0214, Time: 0.64s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.0454, Val Loss: 2.1303, Time: 0.63s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.0396, Val Loss: 2.0827, Time: 0.63s\n",
            "  Result: Acc=0.4797, F1=0.4535\n",
            "\n",
            "=== Final Results ===\n",
            "Mean Accuracy: 0.4465\n"
          ]
        }
      ],
      "source": [
        "# suppose dataset is already a DeepDataset like before\n",
        "input_length = deep_dataset_time[0]['signal'][0].shape[-1]\n",
        "num_classes = len(set([s['metainfo']['label'] for s in deep_dataset_time]))\n",
        "\n",
        "print(f\"Input length: {input_length}, Num classes: {num_classes}\")\n",
        "model = MLP1D_12k(input_length=input_length, num_classes=num_classes)\n",
        "\n",
        "exp = DeepLearningExperiment(\n",
        "    name=\"mlp1d_vibration_12k_optimized\",\n",
        "    description=\"1D MLP for vibration signals\",\n",
        "    dataset=deep_dataset_time,\n",
        "    data_fold_idxs=folds_singleround_deep,  # numpy array of fold indexes\n",
        "    model=model,\n",
        "    batch_size=64,\n",
        "    lr=3e-4,\n",
        "    pretrain_epochs=0,\n",
        "    num_epochs=100\n",
        ")\n",
        "\n",
        "results = exp.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEE_ePYwV08q"
      },
      "source": [
        "### 1D AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "yLqcRhHgWLqV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class AE1D_12k(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementação do Autoencoder 1D Adaptado para 12k pontos.\n",
        "    Arquitetura: 12000 -> 512 -> 256 -> 128 -> 64 (Latent) -> 128 -> 256 -> 512 -> 12000.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_length: int = 12000, latent_dim: int = 64, num_classes: int = 4, dropout_rate: float = 0.4):\n",
        "        super(AE1D_12k, self).__init__()\n",
        "        \n",
        "        # --- Encoder ---\n",
        "        self.encoder = nn.Sequential(\n",
        "            # Camada 1: Compressão Direta (12000 -> 512)\n",
        "            nn.Linear(input_length, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            # Camada 2: 512 -> 256\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            # Camada 3: 256 -> 128\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout_rate),\n",
        "\n",
        "            # Camada Latente: 128 -> 64\n",
        "            nn.Linear(128, latent_dim)\n",
        "        )\n",
        "\n",
        "        # --- Decoder ---\n",
        "        # Simétrico ao encoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            # Latente -> 128\n",
        "            nn.Linear(latent_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # 128 -> 256\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # 256 -> 512\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Reconstrução Final: 512 -> 12000\n",
        "            nn.Linear(512, input_length)\n",
        "        )\n",
        "\n",
        "        # Classificador (Fine-tuning)\n",
        "        self.classifier = nn.Linear(latent_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten para garantir (Batch, 12000)\n",
        "        x = torch.flatten(x, 1)\n",
        "        \n",
        "        # Encoder\n",
        "        latent_features = self.encoder(x)\n",
        "        \n",
        "        # Decoder (Reconstrução do sinal de 12k)\n",
        "        reconstruction = self.decoder(latent_features)\n",
        "        \n",
        "        # Classifier\n",
        "        classification_output = self.classifier(latent_features)\n",
        "        \n",
        "        return classification_output, reconstruction, latent_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "67yzEiLJXKKE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input length: 12000, Num classes: 4\n",
            "\n",
            "=== Outer Fold 1/4 ===\n",
            "[Fold 0] AutoEncoder training (50 epochs)...\n",
            "  [Pre-train] Epoch 1/50 Recon Loss: 0.1972\n",
            "  [Pre-train] Epoch 5/50 Recon Loss: 0.1237\n",
            "  [Pre-train] Epoch 10/50 Recon Loss: 0.1230\n",
            "  [Pre-train] Epoch 15/50 Recon Loss: 0.1231\n",
            "  [Pre-train] Epoch 20/50 Recon Loss: 0.1226\n",
            "  [Pre-train] Epoch 25/50 Recon Loss: 0.1221\n",
            "  [Pre-train] Epoch 30/50 Recon Loss: 0.1227\n",
            "  [Pre-train] Epoch 35/50 Recon Loss: 0.1221\n",
            "  [Pre-train] Epoch 40/50 Recon Loss: 0.1222\n",
            "  [Pre-train] Epoch 45/50 Recon Loss: 0.1222\n",
            "  [Pre-train] Epoch 50/50 Recon Loss: 0.1220\n",
            "[Fold 0] Classifier training (50 epochs)...\n",
            "  [Supervised] Epoch 1/50 Train Loss: 1.3903, Val Loss: 1.3541, Time: 0.19s\n",
            "  [Supervised] Epoch 5/50 Train Loss: 1.0869, Val Loss: 1.2608, Time: 0.20s\n",
            "  [Supervised] Epoch 10/50 Train Loss: 0.3668, Val Loss: 1.1643, Time: 0.17s\n",
            "  [Supervised] Epoch 15/50 Train Loss: 0.1477, Val Loss: 1.2173, Time: 0.17s\n",
            "  [Supervised] Epoch 20/50 Train Loss: 0.0981, Val Loss: 1.3162, Time: 0.17s\n",
            "  [Supervised] Epoch 25/50 Train Loss: 0.1100, Val Loss: 1.4906, Time: 0.17s\n",
            "  [Supervised] Epoch 30/50 Train Loss: 0.0784, Val Loss: 1.5245, Time: 0.17s\n",
            "  [Supervised] Epoch 35/50 Train Loss: 0.0690, Val Loss: 1.5756, Time: 0.17s\n",
            "  [Supervised] Epoch 40/50 Train Loss: 0.0583, Val Loss: 1.6185, Time: 0.17s\n",
            "  [Supervised] Epoch 45/50 Train Loss: 0.0431, Val Loss: 1.6187, Time: 0.17s\n",
            "  [Supervised] Epoch 50/50 Train Loss: 0.0415, Val Loss: 1.6592, Time: 0.17s\n",
            "  Result: Acc=0.3767, F1=0.3670\n",
            "\n",
            "=== Outer Fold 2/4 ===\n",
            "[Fold 1] AutoEncoder training (50 epochs)...\n",
            "  [Pre-train] Epoch 1/50 Recon Loss: 0.2047\n",
            "  [Pre-train] Epoch 5/50 Recon Loss: 0.1303\n",
            "  [Pre-train] Epoch 10/50 Recon Loss: 0.1297\n",
            "  [Pre-train] Epoch 15/50 Recon Loss: 0.1297\n",
            "  [Pre-train] Epoch 20/50 Recon Loss: 0.1295\n",
            "  [Pre-train] Epoch 25/50 Recon Loss: 0.1294\n",
            "  [Pre-train] Epoch 30/50 Recon Loss: 0.1294\n",
            "  [Pre-train] Epoch 35/50 Recon Loss: 0.1295\n",
            "  [Pre-train] Epoch 40/50 Recon Loss: 0.1291\n",
            "  [Pre-train] Epoch 45/50 Recon Loss: 0.1289\n",
            "  [Pre-train] Epoch 50/50 Recon Loss: 0.1287\n",
            "[Fold 1] Classifier training (50 epochs)...\n",
            "  [Supervised] Epoch 1/50 Train Loss: 1.4221, Val Loss: 1.3548, Time: 0.17s\n",
            "  [Supervised] Epoch 5/50 Train Loss: 1.1062, Val Loss: 1.2693, Time: 0.17s\n",
            "  [Supervised] Epoch 10/50 Train Loss: 0.3696, Val Loss: 1.2489, Time: 0.17s\n",
            "  [Supervised] Epoch 15/50 Train Loss: 0.1740, Val Loss: 1.4687, Time: 0.17s\n",
            "  [Supervised] Epoch 20/50 Train Loss: 0.1000, Val Loss: 1.5777, Time: 0.17s\n",
            "  [Supervised] Epoch 25/50 Train Loss: 0.1109, Val Loss: 1.6993, Time: 0.19s\n",
            "  [Supervised] Epoch 30/50 Train Loss: 0.0851, Val Loss: 1.8397, Time: 0.17s\n",
            "  [Supervised] Epoch 35/50 Train Loss: 0.0655, Val Loss: 1.9252, Time: 0.17s\n",
            "  [Supervised] Epoch 40/50 Train Loss: 0.0586, Val Loss: 2.0660, Time: 0.17s\n",
            "  [Supervised] Epoch 45/50 Train Loss: 0.0565, Val Loss: 2.0699, Time: 0.16s\n",
            "  [Supervised] Epoch 50/50 Train Loss: 0.0588, Val Loss: 2.1788, Time: 0.17s\n",
            "  Result: Acc=0.4422, F1=0.4099\n",
            "\n",
            "=== Outer Fold 3/4 ===\n",
            "[Fold 2] AutoEncoder training (50 epochs)...\n",
            "  [Pre-train] Epoch 1/50 Recon Loss: 0.2118\n",
            "  [Pre-train] Epoch 5/50 Recon Loss: 0.1346\n",
            "  [Pre-train] Epoch 10/50 Recon Loss: 0.1340\n",
            "  [Pre-train] Epoch 15/50 Recon Loss: 0.1337\n",
            "  [Pre-train] Epoch 20/50 Recon Loss: 0.1334\n",
            "  [Pre-train] Epoch 25/50 Recon Loss: 0.1337\n",
            "  [Pre-train] Epoch 30/50 Recon Loss: 0.1339\n",
            "  [Pre-train] Epoch 35/50 Recon Loss: 0.1335\n",
            "  [Pre-train] Epoch 40/50 Recon Loss: 0.1335\n",
            "  [Pre-train] Epoch 45/50 Recon Loss: 0.1335\n",
            "  [Pre-train] Epoch 50/50 Recon Loss: 0.1334\n",
            "[Fold 2] Classifier training (50 epochs)...\n",
            "  [Supervised] Epoch 1/50 Train Loss: 1.3859, Val Loss: 1.3594, Time: 0.16s\n",
            "  [Supervised] Epoch 5/50 Train Loss: 1.1084, Val Loss: 1.2969, Time: 0.17s\n",
            "  [Supervised] Epoch 10/50 Train Loss: 0.4087, Val Loss: 1.4148, Time: 0.17s\n",
            "  [Supervised] Epoch 15/50 Train Loss: 0.1927, Val Loss: 1.6037, Time: 0.17s\n",
            "  [Supervised] Epoch 20/50 Train Loss: 0.1356, Val Loss: 1.8003, Time: 0.18s\n",
            "  [Supervised] Epoch 25/50 Train Loss: 0.0862, Val Loss: 1.8243, Time: 0.16s\n",
            "  [Supervised] Epoch 30/50 Train Loss: 0.0836, Val Loss: 1.8867, Time: 0.17s\n",
            "  [Supervised] Epoch 35/50 Train Loss: 0.0642, Val Loss: 1.8857, Time: 0.17s\n",
            "  [Supervised] Epoch 40/50 Train Loss: 0.0716, Val Loss: 1.8942, Time: 0.17s\n",
            "  [Supervised] Epoch 45/50 Train Loss: 0.0416, Val Loss: 2.2805, Time: 0.20s\n",
            "  [Supervised] Epoch 50/50 Train Loss: 0.0302, Val Loss: 2.2489, Time: 0.23s\n",
            "  Result: Acc=0.5078, F1=0.4732\n",
            "\n",
            "=== Outer Fold 4/4 ===\n",
            "[Fold 3] AutoEncoder training (50 epochs)...\n",
            "  [Pre-train] Epoch 1/50 Recon Loss: 0.2053\n",
            "  [Pre-train] Epoch 5/50 Recon Loss: 0.1294\n",
            "  [Pre-train] Epoch 10/50 Recon Loss: 0.1290\n",
            "  [Pre-train] Epoch 15/50 Recon Loss: 0.1287\n",
            "  [Pre-train] Epoch 20/50 Recon Loss: 0.1287\n",
            "  [Pre-train] Epoch 25/50 Recon Loss: 0.1286\n",
            "  [Pre-train] Epoch 30/50 Recon Loss: 0.1285\n",
            "  [Pre-train] Epoch 35/50 Recon Loss: 0.1286\n",
            "  [Pre-train] Epoch 40/50 Recon Loss: 0.1285\n",
            "  [Pre-train] Epoch 45/50 Recon Loss: 0.1281\n",
            "  [Pre-train] Epoch 50/50 Recon Loss: 0.1280\n",
            "[Fold 3] Classifier training (50 epochs)...\n",
            "  [Supervised] Epoch 1/50 Train Loss: 1.3958, Val Loss: 1.3461, Time: 0.19s\n",
            "  [Supervised] Epoch 5/50 Train Loss: 1.0768, Val Loss: 1.2517, Time: 0.21s\n",
            "  [Supervised] Epoch 10/50 Train Loss: 0.3325, Val Loss: 1.2239, Time: 0.17s\n",
            "  [Supervised] Epoch 15/50 Train Loss: 0.1576, Val Loss: 1.3206, Time: 0.17s\n",
            "  [Supervised] Epoch 20/50 Train Loss: 0.1001, Val Loss: 1.4950, Time: 0.17s\n",
            "  [Supervised] Epoch 25/50 Train Loss: 0.0928, Val Loss: 1.6859, Time: 0.17s\n",
            "  [Supervised] Epoch 30/50 Train Loss: 0.0501, Val Loss: 1.9765, Time: 0.17s\n",
            "  [Supervised] Epoch 35/50 Train Loss: 0.0593, Val Loss: 1.9800, Time: 0.17s\n",
            "  [Supervised] Epoch 40/50 Train Loss: 0.0558, Val Loss: 1.9242, Time: 0.17s\n",
            "  [Supervised] Epoch 45/50 Train Loss: 0.0495, Val Loss: 2.1802, Time: 0.18s\n",
            "  [Supervised] Epoch 50/50 Train Loss: 0.0641, Val Loss: 2.1170, Time: 0.16s\n",
            "  Result: Acc=0.4781, F1=0.4617\n",
            "\n",
            "=== Final Results ===\n",
            "Mean Accuracy: 0.4512\n"
          ]
        }
      ],
      "source": [
        "input_length = deep_dataset_time[0]['signal'][0].shape[-1]\n",
        "num_classes = len(set([s['metainfo']['label'] for s in deep_dataset_time]))\n",
        "\n",
        "print(f\"Input length: {input_length}, Num classes: {num_classes}\")\n",
        "model = AE1D_12k(input_length=input_length, latent_dim=64, num_classes=num_classes)\n",
        "\n",
        "# Defina os critérios\n",
        "classification_criterion = nn.CrossEntropyLoss()\n",
        "reconstruction_criterion = nn.MSELoss()\n",
        "\n",
        "exp = DeepLearningExperiment(\n",
        "    name=\"ae1d_vibration_combined_loss\",\n",
        "    description=\"1D AE for vibration signals (Combined Loss)\",\n",
        "    dataset=deep_dataset_time,\n",
        "    data_fold_idxs=folds_singleround_deep,\n",
        "    model=model,\n",
        "    reconstruction_criterion=reconstruction_criterion, # critério para treinamento do autoencoder\n",
        "    criterion=classification_criterion,                # critério para treinamento do classificador\n",
        "    pretrain_epochs=50,  # Treina Encoder+Decoder apenas com MSELoss\n",
        "    num_epochs=50,       # Treina Encoder+Classifier apenas com CrossEntropy\n",
        "    recon_loss_weight=1.0, # recon_loss_weight pode ser 1.0 (pois é a única loss na fase 1)\n",
        "    batch_size=64,\n",
        "    lr=3e-4\n",
        ")\n",
        "\n",
        "results = exp.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOiLccpyM4U0"
      },
      "source": [
        "### 1D Sparse AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Rnpj0Q8oM-1r"
      },
      "outputs": [],
      "source": [
        "class SAE1D_12k(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementação do Sparse Autoencoder 1D adaptado para 12k pontos.\n",
        "    Arquitetura: 12000 -> 512 -> 256 -> 128 -> 64 (Latent).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_length: int = 12000, latent_dim: int = 64, num_classes: int = 4):\n",
        "        super(SAE1D_12k, self).__init__()\n",
        "        \n",
        "        # --- Encoder ---\n",
        "        self.encoder = nn.Sequential(\n",
        "            # Camada 1: Compressão Direta (12000 -> 512)\n",
        "            # Redução drástica necessária para viabilidade\n",
        "            nn.Linear(input_length, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2), # Adicionado Dropout leve\n",
        "\n",
        "            # Camada 2: 512 -> 256\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            # Camada 3: 256 -> 128\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            # Camada Latente: 128 -> Latent Dim\n",
        "            nn.Linear(128, latent_dim)\n",
        "        )\n",
        "\n",
        "        # A Sigmoid é OBRIGATÓRIA para SAE se você usar KL Divergence Loss\n",
        "        # Ela força os neurônios latentes a ficarem entre [0, 1] (probabilidade de ativação)\n",
        "        self.sparsity_activation = nn.Sigmoid()\n",
        "\n",
        "        # --- Decoder ---\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Reconstrução: 512 -> 12000\n",
        "            nn.Linear(512, input_length)\n",
        "        )\n",
        "\n",
        "        # Classificador (Fine-tuning)\n",
        "        self.classifier = nn.Linear(latent_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Garante (Batch, 12000)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # 1. Codificação Linear\n",
        "        features = self.encoder(x)\n",
        "        \n",
        "        # 2. Ativação Esparsa (Latent Space)\n",
        "        # Importante: A saída aqui estará entre 0 e 1\n",
        "        latent_features = self.sparsity_activation(features)\n",
        "\n",
        "        # 3. Reconstrução\n",
        "        reconstruction = self.decoder(latent_features)\n",
        "\n",
        "        # 4. Classificação\n",
        "        classification_output = self.classifier(latent_features)\n",
        "\n",
        "        return classification_output, reconstruction, latent_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input length: 12000, Num classes: 4\n",
            "\n",
            "=== Outer Fold 1/4 ===\n",
            "[Fold 0] AutoEncoder training (50 epochs)...\n",
            "  [Pre-train] Epoch 1/50 Recon Loss: 28.9872\n",
            "  [Pre-train] Epoch 5/50 Recon Loss: 6.1253\n",
            "  [Pre-train] Epoch 10/50 Recon Loss: 0.5986\n",
            "  [Pre-train] Epoch 15/50 Recon Loss: 0.1642\n",
            "  [Pre-train] Epoch 20/50 Recon Loss: 0.1344\n",
            "  [Pre-train] Epoch 25/50 Recon Loss: 0.1340\n",
            "  [Pre-train] Epoch 30/50 Recon Loss: 0.1327\n",
            "  [Pre-train] Epoch 35/50 Recon Loss: 0.1314\n",
            "  [Pre-train] Epoch 40/50 Recon Loss: 0.1330\n",
            "  [Pre-train] Epoch 45/50 Recon Loss: 0.1318\n",
            "  [Pre-train] Epoch 50/50 Recon Loss: 0.1312\n",
            "[Fold 0] Classifier training (50 epochs)...\n",
            "  [Supervised] Epoch 1/50 Train Loss: 1.3772, Val Loss: 1.3769, Time: 0.17s\n",
            "  [Supervised] Epoch 5/50 Train Loss: 1.0628, Val Loss: 1.2937, Time: 0.17s\n",
            "  [Supervised] Epoch 10/50 Train Loss: 0.6577, Val Loss: 1.1826, Time: 0.19s\n",
            "  [Supervised] Epoch 15/50 Train Loss: 0.3536, Val Loss: 1.1826, Time: 0.19s\n",
            "  [Supervised] Epoch 20/50 Train Loss: 0.1849, Val Loss: 1.1947, Time: 0.17s\n",
            "  [Supervised] Epoch 25/50 Train Loss: 0.1275, Val Loss: 1.2994, Time: 0.17s\n",
            "  [Supervised] Epoch 30/50 Train Loss: 0.0903, Val Loss: 1.4529, Time: 0.17s\n",
            "  [Supervised] Epoch 35/50 Train Loss: 0.0719, Val Loss: 1.5446, Time: 0.17s\n",
            "  [Supervised] Epoch 40/50 Train Loss: 0.0531, Val Loss: 1.5436, Time: 0.17s\n",
            "  [Supervised] Epoch 45/50 Train Loss: 0.0403, Val Loss: 1.5803, Time: 0.17s\n",
            "  [Supervised] Epoch 50/50 Train Loss: 0.0427, Val Loss: 1.6518, Time: 0.17s\n",
            "  Result: Acc=0.3300, F1=0.3097\n",
            "\n",
            "=== Outer Fold 2/4 ===\n",
            "[Fold 1] AutoEncoder training (50 epochs)...\n",
            "  [Pre-train] Epoch 1/50 Recon Loss: 29.0314\n",
            "  [Pre-train] Epoch 5/50 Recon Loss: 6.3461\n",
            "  [Pre-train] Epoch 10/50 Recon Loss: 0.5896\n",
            "  [Pre-train] Epoch 15/50 Recon Loss: 0.1857\n",
            "  [Pre-train] Epoch 20/50 Recon Loss: 0.1591\n",
            "  [Pre-train] Epoch 25/50 Recon Loss: 0.1558\n",
            "  [Pre-train] Epoch 30/50 Recon Loss: 0.1533\n",
            "  [Pre-train] Epoch 35/50 Recon Loss: 0.1495\n",
            "  [Pre-train] Epoch 40/50 Recon Loss: 0.1579\n",
            "  [Pre-train] Epoch 45/50 Recon Loss: 0.1490\n",
            "  [Pre-train] Epoch 50/50 Recon Loss: 0.1503\n",
            "[Fold 1] Classifier training (50 epochs)...\n",
            "  [Supervised] Epoch 1/50 Train Loss: 1.3803, Val Loss: 1.3760, Time: 0.17s\n",
            "  [Supervised] Epoch 5/50 Train Loss: 1.0531, Val Loss: 1.3156, Time: 0.17s\n",
            "  [Supervised] Epoch 10/50 Train Loss: 0.7013, Val Loss: 1.2366, Time: 0.17s\n",
            "  [Supervised] Epoch 15/50 Train Loss: 0.3857, Val Loss: 1.1758, Time: 0.17s\n",
            "  [Supervised] Epoch 20/50 Train Loss: 0.2403, Val Loss: 1.2056, Time: 0.18s\n",
            "  [Supervised] Epoch 25/50 Train Loss: 0.2001, Val Loss: 1.4119, Time: 0.19s\n",
            "  [Supervised] Epoch 30/50 Train Loss: 0.1792, Val Loss: 1.3680, Time: 0.21s\n",
            "  [Supervised] Epoch 35/50 Train Loss: 0.1394, Val Loss: 1.4388, Time: 0.17s\n",
            "  [Supervised] Epoch 40/50 Train Loss: 0.1111, Val Loss: 1.4723, Time: 0.17s\n",
            "  [Supervised] Epoch 45/50 Train Loss: 0.0896, Val Loss: 1.5219, Time: 0.17s\n",
            "  [Supervised] Epoch 50/50 Train Loss: 0.0766, Val Loss: 1.6068, Time: 0.17s\n",
            "  Result: Acc=0.4781, F1=0.4705\n",
            "\n",
            "=== Outer Fold 3/4 ===\n",
            "[Fold 2] AutoEncoder training (50 epochs)...\n",
            "  [Pre-train] Epoch 1/50 Recon Loss: 29.0563\n",
            "  [Pre-train] Epoch 5/50 Recon Loss: 6.2146\n",
            "  [Pre-train] Epoch 10/50 Recon Loss: 0.5445\n",
            "  [Pre-train] Epoch 15/50 Recon Loss: 0.1591\n",
            "  [Pre-train] Epoch 20/50 Recon Loss: 0.1474\n",
            "  [Pre-train] Epoch 25/50 Recon Loss: 0.1424\n",
            "  [Pre-train] Epoch 30/50 Recon Loss: 0.1436\n",
            "  [Pre-train] Epoch 35/50 Recon Loss: 0.1390\n",
            "  [Pre-train] Epoch 40/50 Recon Loss: 0.1442\n",
            "  [Pre-train] Epoch 45/50 Recon Loss: 0.1399\n",
            "  [Pre-train] Epoch 50/50 Recon Loss: 0.1399\n",
            "[Fold 2] Classifier training (50 epochs)...\n",
            "  [Supervised] Epoch 1/50 Train Loss: 1.3793, Val Loss: 1.3749, Time: 0.17s\n",
            "  [Supervised] Epoch 5/50 Train Loss: 1.0577, Val Loss: 1.3313, Time: 0.17s\n",
            "  [Supervised] Epoch 10/50 Train Loss: 0.7355, Val Loss: 1.2985, Time: 0.17s\n",
            "  [Supervised] Epoch 15/50 Train Loss: 0.4468, Val Loss: 1.2662, Time: 0.17s\n",
            "  [Supervised] Epoch 20/50 Train Loss: 0.2772, Val Loss: 1.2681, Time: 0.18s\n",
            "  [Supervised] Epoch 25/50 Train Loss: 0.1922, Val Loss: 1.3729, Time: 0.17s\n",
            "  [Supervised] Epoch 30/50 Train Loss: 0.1601, Val Loss: 1.5280, Time: 0.17s\n",
            "  [Supervised] Epoch 35/50 Train Loss: 0.1040, Val Loss: 1.5632, Time: 0.17s\n",
            "  [Supervised] Epoch 40/50 Train Loss: 0.0807, Val Loss: 1.6784, Time: 0.20s\n",
            "  [Supervised] Epoch 45/50 Train Loss: 0.0558, Val Loss: 1.7651, Time: 0.20s\n",
            "  [Supervised] Epoch 50/50 Train Loss: 0.0440, Val Loss: 1.9276, Time: 0.17s\n",
            "  Result: Acc=0.4766, F1=0.4533\n",
            "\n",
            "=== Outer Fold 4/4 ===\n",
            "[Fold 3] AutoEncoder training (50 epochs)...\n",
            "  [Pre-train] Epoch 1/50 Recon Loss: 28.9843\n",
            "  [Pre-train] Epoch 5/50 Recon Loss: 5.8387\n",
            "  [Pre-train] Epoch 10/50 Recon Loss: 0.5297\n",
            "  [Pre-train] Epoch 15/50 Recon Loss: 0.1632\n",
            "  [Pre-train] Epoch 20/50 Recon Loss: 0.1383\n",
            "  [Pre-train] Epoch 25/50 Recon Loss: 0.1371\n",
            "  [Pre-train] Epoch 30/50 Recon Loss: 0.1352\n",
            "  [Pre-train] Epoch 35/50 Recon Loss: 0.1359\n",
            "  [Pre-train] Epoch 40/50 Recon Loss: 0.1358\n",
            "  [Pre-train] Epoch 45/50 Recon Loss: 0.1367\n",
            "  [Pre-train] Epoch 50/50 Recon Loss: 0.1357\n",
            "[Fold 3] Classifier training (50 epochs)...\n",
            "  [Supervised] Epoch 1/50 Train Loss: 1.3819, Val Loss: 1.3682, Time: 0.17s\n",
            "  [Supervised] Epoch 5/50 Train Loss: 1.0558, Val Loss: 1.2216, Time: 0.17s\n",
            "  [Supervised] Epoch 10/50 Train Loss: 0.5096, Val Loss: 1.1428, Time: 0.17s\n",
            "  [Supervised] Epoch 15/50 Train Loss: 0.2854, Val Loss: 1.2186, Time: 0.17s\n",
            "  [Supervised] Epoch 20/50 Train Loss: 0.1945, Val Loss: 1.3200, Time: 0.17s\n",
            "  [Supervised] Epoch 25/50 Train Loss: 0.1028, Val Loss: 1.4361, Time: 0.17s\n",
            "  [Supervised] Epoch 30/50 Train Loss: 0.0780, Val Loss: 1.4980, Time: 0.17s\n",
            "  [Supervised] Epoch 35/50 Train Loss: 0.0644, Val Loss: 1.5302, Time: 0.18s\n",
            "  [Supervised] Epoch 40/50 Train Loss: 0.0513, Val Loss: 1.6446, Time: 0.17s\n",
            "  [Supervised] Epoch 45/50 Train Loss: 0.0383, Val Loss: 1.7500, Time: 0.17s\n",
            "  [Supervised] Epoch 50/50 Train Loss: 0.0310, Val Loss: 1.8004, Time: 0.17s\n",
            "  Result: Acc=0.4797, F1=0.4691\n",
            "\n",
            "=== Final Results ===\n",
            "Mean Accuracy: 0.4411\n"
          ]
        }
      ],
      "source": [
        "input_length = deep_dataset_time[0]['signal'][0].shape[-1]\n",
        "num_classes = len(set([s['metainfo']['label'] for s in deep_dataset_time]))\n",
        "\n",
        "print(f\"Input length: {input_length}, Num classes: {num_classes}\")\n",
        "\n",
        "model = SAE1D_12k(input_length=input_length, latent_dim=64, num_classes=num_classes)\n",
        "\n",
        "# loss criterions\n",
        "classification_criterion = nn.CrossEntropyLoss()\n",
        "reconstruction_criterion = nn.MSELoss()\n",
        "\n",
        "exp = DeepLearningExperiment(\n",
        "    name=\"sae1d_vibration_sequential\",\n",
        "    description=\"1D Sparse AE for vibration signals (Sequential Training)\",\n",
        "    dataset=deep_dataset_time,\n",
        "    data_fold_idxs=folds_singleround_deep,\n",
        "    model=model,\n",
        "    reconstruction_criterion=reconstruction_criterion, # ae train\n",
        "    criterion=classification_criterion,                # classifier train\n",
        "    pretrain_epochs=50,  # Treina Encoder+Decoder com MSE + Esparsidade KL\n",
        "    num_epochs=50,       # Treina Encoder+Classifier com CrossEntropy\n",
        "    sparsity_target=0.05,  # (Rho) Nível de esparsidade desejado (ex: 5% dos neurônios ativos) \n",
        "    sparsity_weight=1.0,   # (Beta) Peso da penalidade KL na perda total\n",
        "    recon_loss_weight=1.0, # Peso da reconstrução (geralmente 1.0 na fase de pré-treino puro)\n",
        "    batch_size=64,\n",
        "    lr=3e-4\n",
        ")\n",
        "\n",
        "results = exp.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1D Denoising AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DAE1D_12k(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementação do Denoising Autoencoder (DAE) adaptado para 12k pontos.\n",
        "    Arquitetura: 12000 -> 512 -> 256 -> 128 -> 64 (Latent).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_length: int = 12000, latent_dim: int = 64, num_classes: int = 4, noise_factor: float = 0.5):\n",
        "        super(DAE1D_12k, self).__init__()\n",
        "        self.noise_factor = noise_factor\n",
        "        \n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            # Camada 1: Compressão Direta (12000 -> 512)\n",
        "            nn.Linear(input_length, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "\n",
        "            # Camada 2: 512 -> 256\n",
        "            nn.Linear(512, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            \n",
        "            # Camada 3: 256 -> 128\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            \n",
        "            # Latent\n",
        "            nn.Linear(128, latent_dim)\n",
        "        )\n",
        "\n",
        "        # Decoder\n",
        "        # Simétrico\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            nn.Linear(128, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            # Reconstrução: 512 -> 12000\n",
        "            nn.Linear(512, input_length)\n",
        "        )\n",
        "\n",
        "        # --- Classificador ---\n",
        "        self.classifier = nn.Linear(latent_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (Batch, 12000)\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # Denoising Injection\n",
        "        if self.training:\n",
        "            # Adiciona ruído apenas durante o treino\n",
        "            noise = torch.randn_like(x) * self.noise_factor\n",
        "            x_noisy = x + noise\n",
        "        else:\n",
        "            x_noisy = x\n",
        "        \n",
        "        # Encoder\n",
        "        latent_features = self.encoder(x_noisy)\n",
        "        \n",
        "        # Decoder\n",
        "        reconstruction = self.decoder(latent_features)\n",
        "        \n",
        "        # Classifier\n",
        "        classification_output = self.classifier(latent_features)\n",
        "\n",
        "        # (Classification, Reconstruction, Latent[Opcional])\n",
        "        return classification_output, reconstruction, latent_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input length: 12000, Num classes: 4\n",
            "\n",
            "=== Outer Fold 1/4 ===\n",
            "[Fold 0] AutoEncoder training (100 epochs)...\n",
            "  [Pre-train] Epoch 1/100 Recon Loss: 0.2216\n",
            "  [Pre-train] Epoch 5/100 Recon Loss: 0.1254\n",
            "  [Pre-train] Epoch 10/100 Recon Loss: 0.1249\n",
            "  [Pre-train] Epoch 15/100 Recon Loss: 0.1245\n",
            "  [Pre-train] Epoch 20/100 Recon Loss: 0.1243\n",
            "  [Pre-train] Epoch 25/100 Recon Loss: 0.1239\n",
            "  [Pre-train] Epoch 30/100 Recon Loss: 0.1235\n",
            "  [Pre-train] Epoch 35/100 Recon Loss: 0.1227\n",
            "  [Pre-train] Epoch 40/100 Recon Loss: 0.1225\n",
            "  [Pre-train] Epoch 45/100 Recon Loss: 0.1216\n",
            "  [Pre-train] Epoch 50/100 Recon Loss: 0.1201\n",
            "  [Pre-train] Epoch 55/100 Recon Loss: 0.1191\n",
            "  [Pre-train] Epoch 60/100 Recon Loss: 0.1185\n",
            "  [Pre-train] Epoch 65/100 Recon Loss: 0.1155\n",
            "  [Pre-train] Epoch 70/100 Recon Loss: 0.1132\n",
            "  [Pre-train] Epoch 75/100 Recon Loss: 0.1101\n",
            "  [Pre-train] Epoch 80/100 Recon Loss: 0.1081\n",
            "  [Pre-train] Epoch 85/100 Recon Loss: 0.1051\n",
            "  [Pre-train] Epoch 90/100 Recon Loss: 0.1032\n",
            "  [Pre-train] Epoch 95/100 Recon Loss: 0.0999\n",
            "  [Pre-train] Epoch 100/100 Recon Loss: 0.0956\n",
            "[Fold 0] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.3928, Val Loss: 1.3418, Time: 0.17s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 1.2033, Val Loss: 1.3027, Time: 0.18s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.9484, Val Loss: 1.3214, Time: 0.17s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.8646, Val Loss: 1.2982, Time: 0.17s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.7847, Val Loss: 1.2867, Time: 0.17s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.7498, Val Loss: 1.2392, Time: 0.17s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.6891, Val Loss: 1.2527, Time: 0.18s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.6620, Val Loss: 1.2033, Time: 0.20s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.6161, Val Loss: 1.1749, Time: 0.22s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.5782, Val Loss: 1.1898, Time: 0.18s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.5553, Val Loss: 1.0798, Time: 0.18s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.5491, Val Loss: 1.0555, Time: 0.17s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.5055, Val Loss: 1.0022, Time: 0.17s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.4352, Val Loss: 0.9493, Time: 0.17s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.4678, Val Loss: 0.9097, Time: 0.17s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.4532, Val Loss: 0.9163, Time: 0.18s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.4241, Val Loss: 0.9315, Time: 0.18s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.4112, Val Loss: 0.9411, Time: 0.17s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.3921, Val Loss: 0.9417, Time: 0.17s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.3965, Val Loss: 0.9533, Time: 0.17s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.3849, Val Loss: 0.9474, Time: 0.19s\n",
            "  Result: Acc=0.4083, F1=0.3837\n",
            "\n",
            "=== Outer Fold 2/4 ===\n",
            "[Fold 1] AutoEncoder training (100 epochs)...\n",
            "  [Pre-train] Epoch 1/100 Recon Loss: 0.2402\n",
            "  [Pre-train] Epoch 5/100 Recon Loss: 0.1453\n",
            "  [Pre-train] Epoch 10/100 Recon Loss: 0.1445\n",
            "  [Pre-train] Epoch 15/100 Recon Loss: 0.1439\n",
            "  [Pre-train] Epoch 20/100 Recon Loss: 0.1438\n",
            "  [Pre-train] Epoch 25/100 Recon Loss: 0.1434\n",
            "  [Pre-train] Epoch 30/100 Recon Loss: 0.1424\n",
            "  [Pre-train] Epoch 35/100 Recon Loss: 0.1415\n",
            "  [Pre-train] Epoch 40/100 Recon Loss: 0.1394\n",
            "  [Pre-train] Epoch 45/100 Recon Loss: 0.1387\n",
            "  [Pre-train] Epoch 50/100 Recon Loss: 0.1373\n",
            "  [Pre-train] Epoch 55/100 Recon Loss: 0.1354\n",
            "  [Pre-train] Epoch 60/100 Recon Loss: 0.1330\n",
            "  [Pre-train] Epoch 65/100 Recon Loss: 0.1328\n",
            "  [Pre-train] Epoch 70/100 Recon Loss: 0.1318\n",
            "  [Pre-train] Epoch 75/100 Recon Loss: 0.1293\n",
            "  [Pre-train] Epoch 80/100 Recon Loss: 0.1277\n",
            "  [Pre-train] Epoch 85/100 Recon Loss: 0.1274\n",
            "  [Pre-train] Epoch 90/100 Recon Loss: 0.1246\n",
            "  [Pre-train] Epoch 95/100 Recon Loss: 0.1229\n",
            "  [Pre-train] Epoch 100/100 Recon Loss: 0.1197\n",
            "[Fold 1] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.3588, Val Loss: 1.3376, Time: 0.17s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 1.1945, Val Loss: 1.3038, Time: 0.17s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.9763, Val Loss: 1.4114, Time: 0.19s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.8623, Val Loss: 1.5261, Time: 0.19s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.8308, Val Loss: 1.5816, Time: 0.17s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.8140, Val Loss: 1.6104, Time: 0.17s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.7659, Val Loss: 1.5654, Time: 0.18s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.7356, Val Loss: 1.5762, Time: 0.17s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.6802, Val Loss: 1.5877, Time: 0.17s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.6850, Val Loss: 1.3999, Time: 0.17s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.6803, Val Loss: 1.4578, Time: 0.17s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.6298, Val Loss: 1.3444, Time: 0.17s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.5849, Val Loss: 1.2884, Time: 0.18s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.5671, Val Loss: 1.2233, Time: 0.17s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.5511, Val Loss: 1.1843, Time: 0.17s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.5065, Val Loss: 1.1614, Time: 0.20s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.5100, Val Loss: 1.1716, Time: 0.21s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.4664, Val Loss: 1.1527, Time: 0.17s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.4473, Val Loss: 1.1614, Time: 0.17s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.4469, Val Loss: 1.1321, Time: 0.18s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.4522, Val Loss: 1.0885, Time: 0.17s\n",
            "  Result: Acc=0.4266, F1=0.3512\n",
            "\n",
            "=== Outer Fold 3/4 ===\n",
            "[Fold 2] AutoEncoder training (100 epochs)...\n",
            "  [Pre-train] Epoch 1/100 Recon Loss: 0.2296\n",
            "  [Pre-train] Epoch 5/100 Recon Loss: 0.1366\n",
            "  [Pre-train] Epoch 10/100 Recon Loss: 0.1357\n",
            "  [Pre-train] Epoch 15/100 Recon Loss: 0.1349\n",
            "  [Pre-train] Epoch 20/100 Recon Loss: 0.1345\n",
            "  [Pre-train] Epoch 25/100 Recon Loss: 0.1339\n",
            "  [Pre-train] Epoch 30/100 Recon Loss: 0.1333\n",
            "  [Pre-train] Epoch 35/100 Recon Loss: 0.1328\n",
            "  [Pre-train] Epoch 40/100 Recon Loss: 0.1321\n",
            "  [Pre-train] Epoch 45/100 Recon Loss: 0.1312\n",
            "  [Pre-train] Epoch 50/100 Recon Loss: 0.1292\n",
            "  [Pre-train] Epoch 55/100 Recon Loss: 0.1273\n",
            "  [Pre-train] Epoch 60/100 Recon Loss: 0.1265\n",
            "  [Pre-train] Epoch 65/100 Recon Loss: 0.1215\n",
            "  [Pre-train] Epoch 70/100 Recon Loss: 0.1198\n",
            "  [Pre-train] Epoch 75/100 Recon Loss: 0.1165\n",
            "  [Pre-train] Epoch 80/100 Recon Loss: 0.1151\n",
            "  [Pre-train] Epoch 85/100 Recon Loss: 0.1115\n",
            "  [Pre-train] Epoch 90/100 Recon Loss: 0.1071\n",
            "  [Pre-train] Epoch 95/100 Recon Loss: 0.1041\n",
            "  [Pre-train] Epoch 100/100 Recon Loss: 0.1030\n",
            "[Fold 2] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.3421, Val Loss: 1.3395, Time: 0.17s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 1.1834, Val Loss: 1.3127, Time: 0.17s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.9495, Val Loss: 1.4164, Time: 0.17s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.8870, Val Loss: 1.4795, Time: 0.17s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.8294, Val Loss: 1.5354, Time: 0.17s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.7817, Val Loss: 1.4785, Time: 0.17s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.7499, Val Loss: 1.4548, Time: 0.17s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.7339, Val Loss: 1.4190, Time: 0.18s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.6893, Val Loss: 1.4068, Time: 0.17s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.6731, Val Loss: 1.4150, Time: 0.17s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.6409, Val Loss: 1.3722, Time: 0.20s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.5844, Val Loss: 1.3672, Time: 0.21s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.5755, Val Loss: 1.3194, Time: 0.17s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.5779, Val Loss: 1.3217, Time: 0.17s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.5451, Val Loss: 1.3427, Time: 0.18s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.5250, Val Loss: 1.3305, Time: 0.17s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.5006, Val Loss: 1.3250, Time: 0.17s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.4908, Val Loss: 1.2844, Time: 0.17s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.5118, Val Loss: 1.2843, Time: 0.17s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.4781, Val Loss: 1.2781, Time: 0.17s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.4405, Val Loss: 1.2834, Time: 0.18s\n",
            "  Result: Acc=0.4672, F1=0.3904\n",
            "\n",
            "=== Outer Fold 4/4 ===\n",
            "[Fold 3] AutoEncoder training (100 epochs)...\n",
            "  [Pre-train] Epoch 1/100 Recon Loss: 0.2217\n",
            "  [Pre-train] Epoch 5/100 Recon Loss: 0.1240\n",
            "  [Pre-train] Epoch 10/100 Recon Loss: 0.1237\n",
            "  [Pre-train] Epoch 15/100 Recon Loss: 0.1231\n",
            "  [Pre-train] Epoch 20/100 Recon Loss: 0.1230\n",
            "  [Pre-train] Epoch 25/100 Recon Loss: 0.1229\n",
            "  [Pre-train] Epoch 30/100 Recon Loss: 0.1223\n",
            "  [Pre-train] Epoch 35/100 Recon Loss: 0.1222\n",
            "  [Pre-train] Epoch 40/100 Recon Loss: 0.1212\n",
            "  [Pre-train] Epoch 45/100 Recon Loss: 0.1208\n",
            "  [Pre-train] Epoch 50/100 Recon Loss: 0.1203\n",
            "  [Pre-train] Epoch 55/100 Recon Loss: 0.1194\n",
            "  [Pre-train] Epoch 60/100 Recon Loss: 0.1179\n",
            "  [Pre-train] Epoch 65/100 Recon Loss: 0.1169\n",
            "  [Pre-train] Epoch 70/100 Recon Loss: 0.1155\n",
            "  [Pre-train] Epoch 75/100 Recon Loss: 0.1149\n",
            "  [Pre-train] Epoch 80/100 Recon Loss: 0.1125\n",
            "  [Pre-train] Epoch 85/100 Recon Loss: 0.1112\n",
            "  [Pre-train] Epoch 90/100 Recon Loss: 0.1094\n",
            "  [Pre-train] Epoch 95/100 Recon Loss: 0.1081\n",
            "  [Pre-train] Epoch 100/100 Recon Loss: 0.1053\n",
            "[Fold 3] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.3674, Val Loss: 1.3327, Time: 0.17s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 1.1923, Val Loss: 1.3058, Time: 0.17s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.9623, Val Loss: 1.3371, Time: 0.17s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.8970, Val Loss: 1.3774, Time: 0.18s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.8121, Val Loss: 1.3408, Time: 0.17s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.8152, Val Loss: 1.2729, Time: 0.20s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.7286, Val Loss: 1.3161, Time: 0.20s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.7003, Val Loss: 1.3094, Time: 0.17s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.6643, Val Loss: 1.3234, Time: 0.17s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.6393, Val Loss: 1.2918, Time: 0.17s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.6003, Val Loss: 1.2270, Time: 0.18s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.5757, Val Loss: 1.2944, Time: 0.17s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.5688, Val Loss: 1.2787, Time: 0.17s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.5540, Val Loss: 1.1955, Time: 0.17s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.5275, Val Loss: 1.1815, Time: 0.17s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.5048, Val Loss: 1.0801, Time: 0.17s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.4805, Val Loss: 1.1352, Time: 0.18s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.4575, Val Loss: 1.1029, Time: 0.17s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.4555, Val Loss: 1.0684, Time: 0.17s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.4425, Val Loss: 1.0725, Time: 0.19s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.4102, Val Loss: 1.0524, Time: 0.17s\n",
            "  Result: Acc=0.4391, F1=0.4042\n",
            "\n",
            "=== Final Results ===\n",
            "Mean Accuracy: 0.4353\n"
          ]
        }
      ],
      "source": [
        "input_length = deep_dataset_time[0]['signal'][0].shape[-1]\n",
        "num_classes = len(set([s['metainfo']['label'] for s in deep_dataset_time]))\n",
        "\n",
        "print(f\"Input length: {input_length}, Num classes: {num_classes}\")\n",
        "\n",
        "# noise_factor controla o ruído\n",
        "model = DAE1D_12k(input_length=input_length, latent_dim=64, num_classes=num_classes, noise_factor=0.5)\n",
        "\n",
        "classification_criterion = nn.CrossEntropyLoss()\n",
        "reconstruction_criterion = nn.MSELoss()\n",
        "\n",
        "# 4. Configuração do Experimento\n",
        "exp = DeepLearningExperiment(\n",
        "    name=\"dae1d_vibration_sequential\",\n",
        "    description=\"1D Denoising AE for vibration signals (Sequential Training)\",\n",
        "    dataset=deep_dataset_time,\n",
        "    data_fold_idxs=folds_singleround_deep,\n",
        "    model=model,\n",
        "    reconstruction_criterion=reconstruction_criterion, # ae train\n",
        "    criterion=classification_criterion,                # classifier train\n",
        "    pretrain_epochs=100,  # Treina Encoder+Decoder com MSELoss (DAE)\n",
        "    num_epochs=100,       # Treina Encoder+Classifier com CrossEntropy\n",
        "    recon_loss_weight=1.0,\n",
        "    batch_size=64, # Batch grande ajuda na estabilidade do AE\n",
        "    lr=3e-4\n",
        ")\n",
        "\n",
        "# 5. Execução\n",
        "results = exp.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1D CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CNN1D(nn.Module):\n",
        "    def __init__(self, input_length: int, num_classes: int):\n",
        "        \"\"\"\n",
        "        1D CNN for vibration signal classification.\n",
        "        Args:\n",
        "            input_length: length of the input signal\n",
        "            num_classes: number of output classes\n",
        "        \"\"\"\n",
        "        super(CNN1D, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=7, padding=3)\n",
        "        self.bn1 = nn.BatchNorm1d(16)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(16, 32, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "        self.pool2 = nn.MaxPool1d(2)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm1d(64)\n",
        "        self.pool3 = nn.AdaptiveMaxPool1d(16)  # reduce dynamically to fixed size\n",
        "\n",
        "        # compute flattened size\n",
        "        example_input = torch.zeros(1, 1, input_length)  # [B, C, L]\n",
        "        with torch.no_grad():\n",
        "            x = self.pool1(F.relu(self.bn1(self.conv1(example_input))))\n",
        "            x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
        "            x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
        "            flattened_size = x.shape[1] * x.shape[2]\n",
        "\n",
        "        self.fc1 = nn.Linear(flattened_size, 128)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [B, L] or [B, 1, L]\n",
        "        if x.ndim == 2:\n",
        "            x = x.unsqueeze(1)  # add channel dim\n",
        "\n",
        "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input length: 12000, Num classes: 4\n",
            "\n",
            "=== Outer Fold 1/4 ===\n",
            "[Fold 0] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.2839, Val Loss: 1.2835, Time: 1.61s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 0.8785, Val Loss: 0.8869, Time: 0.68s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.6360, Val Loss: 0.6819, Time: 0.69s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.5043, Val Loss: 0.5721, Time: 0.70s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.3873, Val Loss: 0.3890, Time: 0.70s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.3099, Val Loss: 0.3440, Time: 0.70s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.2395, Val Loss: 0.2454, Time: 0.71s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.1766, Val Loss: 0.1565, Time: 0.69s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.1490, Val Loss: 0.1397, Time: 0.69s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.1312, Val Loss: 0.2100, Time: 0.68s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.0802, Val Loss: 0.1138, Time: 0.68s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.1020, Val Loss: 0.0991, Time: 0.68s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.0969, Val Loss: 0.0736, Time: 0.68s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.0629, Val Loss: 0.0607, Time: 0.68s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.0467, Val Loss: 0.0518, Time: 0.68s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.0476, Val Loss: 0.0708, Time: 0.67s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.0406, Val Loss: 0.0475, Time: 0.67s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.0415, Val Loss: 0.0554, Time: 0.67s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.0454, Val Loss: 0.0453, Time: 0.67s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.0243, Val Loss: 0.0380, Time: 0.67s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.0273, Val Loss: 0.0333, Time: 0.68s\n",
            "  Result: Acc=0.7833, F1=0.7812\n",
            "\n",
            "=== Outer Fold 2/4 ===\n",
            "[Fold 1] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.3218, Val Loss: 1.2532, Time: 0.66s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 0.9378, Val Loss: 0.9372, Time: 0.66s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.6980, Val Loss: 0.7056, Time: 0.66s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.6197, Val Loss: 0.5849, Time: 0.66s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.4915, Val Loss: 0.5179, Time: 0.66s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.3564, Val Loss: 0.4047, Time: 0.67s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.3065, Val Loss: 0.3229, Time: 0.67s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.2742, Val Loss: 0.3105, Time: 0.68s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.2270, Val Loss: 0.2723, Time: 0.67s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.1725, Val Loss: 0.2267, Time: 0.69s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.1566, Val Loss: 0.2038, Time: 0.67s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.1280, Val Loss: 0.2003, Time: 0.67s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.1190, Val Loss: 0.1666, Time: 0.67s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.1035, Val Loss: 0.1690, Time: 0.67s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.1191, Val Loss: 0.1440, Time: 0.68s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.0883, Val Loss: 0.2077, Time: 0.67s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.0820, Val Loss: 0.1309, Time: 0.67s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.0868, Val Loss: 0.1324, Time: 0.67s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.0526, Val Loss: 0.1513, Time: 0.67s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.1375, Val Loss: 0.1291, Time: 0.67s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.0594, Val Loss: 0.1376, Time: 0.67s\n",
            "  Result: Acc=0.9578, F1=0.9576\n",
            "\n",
            "=== Outer Fold 3/4 ===\n",
            "[Fold 2] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.2684, Val Loss: 1.2451, Time: 0.66s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 0.9117, Val Loss: 0.8488, Time: 0.68s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.6733, Val Loss: 0.7022, Time: 0.66s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.5576, Val Loss: 0.5557, Time: 0.66s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.5103, Val Loss: 0.4778, Time: 0.67s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.3681, Val Loss: 0.3594, Time: 0.66s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.3141, Val Loss: 0.3153, Time: 0.67s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.2509, Val Loss: 0.2658, Time: 0.66s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.2087, Val Loss: 0.2378, Time: 0.67s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.1899, Val Loss: 0.2168, Time: 0.67s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.1491, Val Loss: 0.1811, Time: 0.67s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.1449, Val Loss: 0.1594, Time: 0.67s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.1270, Val Loss: 0.1590, Time: 0.67s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.1209, Val Loss: 0.2210, Time: 0.67s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.1319, Val Loss: 0.1845, Time: 0.67s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.1136, Val Loss: 0.1600, Time: 0.68s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.0830, Val Loss: 0.1087, Time: 0.67s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.0858, Val Loss: 0.0962, Time: 0.67s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.0792, Val Loss: 0.1051, Time: 0.67s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.0804, Val Loss: 0.0887, Time: 0.67s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.0591, Val Loss: 0.1038, Time: 0.67s\n",
            "  Result: Acc=0.9734, F1=0.9734\n",
            "\n",
            "=== Outer Fold 4/4 ===\n",
            "[Fold 3] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.2694, Val Loss: 1.2343, Time: 0.66s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 0.8669, Val Loss: 0.7767, Time: 0.67s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.6587, Val Loss: 0.5719, Time: 0.68s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.5094, Val Loss: 0.4351, Time: 0.67s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.4027, Val Loss: 0.3573, Time: 0.67s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.3240, Val Loss: 0.2898, Time: 0.67s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.2564, Val Loss: 0.2924, Time: 0.67s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.3187, Val Loss: 0.3969, Time: 0.67s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.1988, Val Loss: 0.1866, Time: 0.67s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.1871, Val Loss: 0.1968, Time: 0.67s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.1522, Val Loss: 0.1871, Time: 0.66s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.1242, Val Loss: 0.1331, Time: 0.66s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.1116, Val Loss: 0.1156, Time: 0.66s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.0951, Val Loss: 0.0889, Time: 0.67s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.0963, Val Loss: 0.0897, Time: 0.67s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.0716, Val Loss: 0.0854, Time: 0.66s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.0599, Val Loss: 0.0722, Time: 0.67s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.0627, Val Loss: 0.0867, Time: 0.67s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.0448, Val Loss: 0.0691, Time: 0.67s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.0579, Val Loss: 0.0506, Time: 0.67s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.0382, Val Loss: 0.0507, Time: 0.67s\n",
            "  Result: Acc=0.8734, F1=0.8708\n",
            "\n",
            "=== Final Results ===\n",
            "Mean Accuracy: 0.8970\n"
          ]
        }
      ],
      "source": [
        "# suppose dataset is already a DeepDataset like before\n",
        "input_length = deep_dataset_time[0]['signal'][0].shape[-1]\n",
        "num_classes = len(set([s['metainfo']['label'] for s in deep_dataset_time]))\n",
        "\n",
        "print(f\"Input length: {input_length}, Num classes: {num_classes}\")\n",
        "model = CNN1D(input_length=input_length, num_classes=num_classes)\n",
        "\n",
        "exp = DeepLearningExperiment(\n",
        "    name=\"cnn1d_vibration\",\n",
        "    description=\"1D CNN for vibration signals\",\n",
        "    dataset=deep_dataset_time,\n",
        "    data_fold_idxs=folds_singleround_deep,  # numpy array of fold indexes\n",
        "    model=model,\n",
        "    batch_size=64,\n",
        "    lr=3e-4,\n",
        "    num_epochs=100\n",
        ")\n",
        "\n",
        "results = exp.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1D LeNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LeNet1D_12k(nn.Module):\n",
        "    def __init__(self, in_channel=1, out_channel=4):\n",
        "        super(LeNet1D_12k, self).__init__()\n",
        "        \n",
        "        # --- Bloco Convolucional 1 ---\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv1d(in_channel, 6, kernel_size=64, stride=4, padding=30),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2), # 3000 -> 1500\n",
        "        )\n",
        "        \n",
        "        # --- Bloco Convolucional 2 ---\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv1d(6, 16, kernel_size=5, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveMaxPool1d(12) \n",
        "        )\n",
        "        \n",
        "        # --- Classificador (Fully Connected) ---\n",
        "        # Input achatado: 16 canais * 12 pontos = 192 features\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16 * 12, 120), # Tamanho clássico da LeNet-5\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(120, 84), # Tamanho clássico da LeNet-5\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        self.fc3 = nn.Linear(84, out_channel)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Garante dimensão de canal (Batch, 1, 12000)\n",
        "        if x.ndim == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "            \n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input length: 12000, Num classes: 4\n",
            "\n",
            "=== Outer Fold 1/4 ===\n",
            "[Fold 0] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.3751, Val Loss: 1.3513, Time: 0.15s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 1.2038, Val Loss: 1.1826, Time: 0.12s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.9738, Val Loss: 0.9340, Time: 0.14s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.8127, Val Loss: 0.8063, Time: 0.11s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.6943, Val Loss: 0.6757, Time: 0.11s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.6077, Val Loss: 0.5942, Time: 0.10s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.5106, Val Loss: 0.5386, Time: 0.11s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.4391, Val Loss: 0.4412, Time: 0.10s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.3708, Val Loss: 0.3657, Time: 0.11s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.3166, Val Loss: 0.3360, Time: 0.10s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.2801, Val Loss: 0.3066, Time: 0.11s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.2407, Val Loss: 0.2862, Time: 0.10s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.2046, Val Loss: 0.2469, Time: 0.10s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.1824, Val Loss: 0.2352, Time: 0.10s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.1889, Val Loss: 0.1893, Time: 0.10s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.1390, Val Loss: 0.1881, Time: 0.10s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.1297, Val Loss: 0.1712, Time: 0.10s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.1275, Val Loss: 0.1523, Time: 0.11s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.0915, Val Loss: 0.1896, Time: 0.10s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.0867, Val Loss: 0.1289, Time: 0.11s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.0773, Val Loss: 0.1415, Time: 0.11s\n",
            "  Result: Acc=0.6400, F1=0.6413\n",
            "\n",
            "=== Outer Fold 2/4 ===\n",
            "[Fold 1] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.3744, Val Loss: 1.3447, Time: 0.11s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 1.1940, Val Loss: 1.1695, Time: 0.14s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.9940, Val Loss: 0.9765, Time: 0.12s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.8399, Val Loss: 0.8312, Time: 0.13s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.7415, Val Loss: 0.7867, Time: 0.11s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.6637, Val Loss: 0.6790, Time: 0.10s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.5956, Val Loss: 0.6234, Time: 0.10s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.5164, Val Loss: 0.5746, Time: 0.10s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.4542, Val Loss: 0.4899, Time: 0.10s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.4126, Val Loss: 0.4271, Time: 0.10s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.3580, Val Loss: 0.4008, Time: 0.10s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.3578, Val Loss: 0.3715, Time: 0.10s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.2929, Val Loss: 0.3438, Time: 0.10s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.2685, Val Loss: 0.3188, Time: 0.10s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.2484, Val Loss: 0.2979, Time: 0.10s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.2260, Val Loss: 0.3065, Time: 0.10s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.2045, Val Loss: 0.2716, Time: 0.10s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.1836, Val Loss: 0.2601, Time: 0.11s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.1704, Val Loss: 0.2571, Time: 0.10s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.1593, Val Loss: 0.2447, Time: 0.10s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.1442, Val Loss: 0.2366, Time: 0.10s\n",
            "  Result: Acc=0.8781, F1=0.8743\n",
            "\n",
            "=== Outer Fold 3/4 ===\n",
            "[Fold 2] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.3756, Val Loss: 1.3488, Time: 0.10s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 1.1987, Val Loss: 1.1761, Time: 0.11s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 1.0112, Val Loss: 0.9800, Time: 0.15s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.8608, Val Loss: 0.8445, Time: 0.12s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.7586, Val Loss: 0.7632, Time: 0.13s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.6823, Val Loss: 0.6835, Time: 0.11s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.6096, Val Loss: 0.6675, Time: 0.10s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.5418, Val Loss: 0.5790, Time: 0.11s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.4913, Val Loss: 0.5446, Time: 0.10s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.4386, Val Loss: 0.4869, Time: 0.11s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.3944, Val Loss: 0.4341, Time: 0.10s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.3540, Val Loss: 0.4076, Time: 0.11s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.3166, Val Loss: 0.3982, Time: 0.10s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.2902, Val Loss: 0.3542, Time: 0.11s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.2751, Val Loss: 0.3429, Time: 0.10s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.2499, Val Loss: 0.3260, Time: 0.11s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.2381, Val Loss: 0.3254, Time: 0.10s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.2131, Val Loss: 0.3012, Time: 0.11s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.2019, Val Loss: 0.2896, Time: 0.10s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.1893, Val Loss: 0.2800, Time: 0.11s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.1749, Val Loss: 0.2750, Time: 0.10s\n",
            "  Result: Acc=0.8953, F1=0.8942\n",
            "\n",
            "=== Outer Fold 4/4 ===\n",
            "[Fold 3] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.3747, Val Loss: 1.3550, Time: 0.11s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 1.1938, Val Loss: 1.1967, Time: 0.10s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.9878, Val Loss: 1.0001, Time: 0.10s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.8280, Val Loss: 0.8571, Time: 0.10s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.7033, Val Loss: 0.7778, Time: 0.14s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.6235, Val Loss: 0.6716, Time: 0.12s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.5356, Val Loss: 0.6392, Time: 0.11s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.4735, Val Loss: 0.5573, Time: 0.10s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.4332, Val Loss: 0.5382, Time: 0.10s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.3778, Val Loss: 0.4973, Time: 0.11s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.3509, Val Loss: 0.4562, Time: 0.10s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.3152, Val Loss: 0.5051, Time: 0.10s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.2857, Val Loss: 0.3783, Time: 0.10s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.2766, Val Loss: 0.3571, Time: 0.10s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.2408, Val Loss: 0.3365, Time: 0.10s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.2308, Val Loss: 0.3602, Time: 0.11s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.2085, Val Loss: 0.2965, Time: 0.10s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.2048, Val Loss: 0.2854, Time: 0.10s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.1816, Val Loss: 0.2863, Time: 0.10s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.1772, Val Loss: 0.2809, Time: 0.10s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.1704, Val Loss: 0.2953, Time: 0.10s\n",
            "  Result: Acc=0.8000, F1=0.7966\n",
            "\n",
            "=== Final Results ===\n",
            "Mean Accuracy: 0.8034\n"
          ]
        }
      ],
      "source": [
        "# suppose dataset is already a DeepDataset like before\n",
        "input_length = deep_dataset_time[0]['signal'][0].shape[-1]\n",
        "num_classes = len(set([s['metainfo']['label'] for s in deep_dataset_time]))\n",
        "\n",
        "print(f\"Input length: {input_length}, Num classes: {num_classes}\")\n",
        "model = LeNet1D_12k(in_channel=1, out_channel=num_classes)\n",
        "\n",
        "exp = DeepLearningExperiment(\n",
        "    name=\"1d_lenet vibration\",\n",
        "    description=\"1D LeNet for vibration signals\",\n",
        "    dataset=deep_dataset_time,\n",
        "    data_fold_idxs=folds_singleround_deep,  # numpy array of fold indexes\n",
        "    model=model,\n",
        "    batch_size=64,\n",
        "    lr=3e-4,\n",
        "    num_epochs=100\n",
        ")\n",
        "\n",
        "results = exp.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1D ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def conv3x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x1 convolution with padding\"\"\"\n",
        "    return nn.Conv1d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    \"\"\"1x1 convolution\"\"\"\n",
        "    return nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x1(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm1d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x1(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm1d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet18_12k(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementação da ResNet-18 adaptada para sinais 1D de 12.000 pontos.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_length=12000, in_channel=1, num_classes=10):\n",
        "        super(ResNet18_12k, self).__init__()\n",
        "        \n",
        "        # Configuração padrão da ResNet18\n",
        "        block = BasicBlock\n",
        "        layers = [2, 2, 2, 2] # 2 blocos por camada = 18 layers total\n",
        "        \n",
        "        self.inplanes = 64\n",
        "        \n",
        "        # STEM (Camada de Entrada)\n",
        "        # Input: (Batch, 1, 12000)\n",
        "        self.conv1 = nn.Conv1d(in_channel, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        # Blocos Residuais\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        \n",
        "        # Classificador\n",
        "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        # Inicialização de Pesos\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv1d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm1d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm1d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Tratamento de Dimensão: Garante (Batch, 1, Length)\n",
        "        if x.ndim == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "\n",
        "        # Stem\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # Layers\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        # Head\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input length: 12000, Num classes: 4\n",
            "\n",
            "=== Outer Fold 1/4 ===\n",
            "[Fold 0] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.0428, Val Loss: 3.0452, Time: 9.00s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 0.2921, Val Loss: 0.4500, Time: 8.33s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.0956, Val Loss: 0.0389, Time: 8.44s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.0764, Val Loss: 0.0379, Time: 8.42s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.0698, Val Loss: 0.1219, Time: 8.41s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.0057, Val Loss: 0.0176, Time: 8.45s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.0104, Val Loss: 0.0897, Time: 8.44s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.0055, Val Loss: 0.0166, Time: 8.48s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.0146, Val Loss: 0.0077, Time: 8.40s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.0013, Val Loss: 0.0034, Time: 8.43s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.0018, Val Loss: 0.0068, Time: 8.47s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.0004, Val Loss: 0.0045, Time: 8.48s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.0004, Val Loss: 0.0047, Time: 8.47s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.0003, Val Loss: 0.0065, Time: 8.46s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.1347, Val Loss: 1.7753, Time: 8.42s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.0210, Val Loss: 0.0138, Time: 8.43s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.0009, Val Loss: 0.0094, Time: 8.43s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.0015, Val Loss: 0.0093, Time: 8.43s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.0006, Val Loss: 0.0113, Time: 8.44s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.0009, Val Loss: 0.0122, Time: 8.47s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.0003, Val Loss: 0.0154, Time: 8.46s\n",
            "  Result: Acc=0.7483, F1=0.7363\n",
            "\n",
            "=== Outer Fold 2/4 ===\n",
            "[Fold 1] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 0.9461, Val Loss: 1.7967, Time: 8.27s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 0.3648, Val Loss: 0.3650, Time: 8.26s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.1442, Val Loss: 0.4173, Time: 8.28s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.0923, Val Loss: 0.1293, Time: 8.28s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.0724, Val Loss: 0.1150, Time: 8.24s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.0392, Val Loss: 0.0591, Time: 8.24s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.0703, Val Loss: 0.0809, Time: 8.25s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.0304, Val Loss: 0.0301, Time: 8.27s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.0296, Val Loss: 0.0176, Time: 8.26s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.0228, Val Loss: 0.0853, Time: 8.27s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.0127, Val Loss: 0.0223, Time: 8.28s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.0174, Val Loss: 0.0434, Time: 8.30s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.0278, Val Loss: 0.0132, Time: 8.29s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.0214, Val Loss: 0.0664, Time: 8.30s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.0036, Val Loss: 0.0039, Time: 8.30s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.0007, Val Loss: 0.0081, Time: 8.31s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.0007, Val Loss: 0.0063, Time: 8.31s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.0008, Val Loss: 0.0023, Time: 8.32s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.0094, Val Loss: 0.0461, Time: 8.32s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.0004, Val Loss: 0.0022, Time: 8.31s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.0007, Val Loss: 0.0052, Time: 8.31s\n",
            "  Result: Acc=0.9922, F1=0.9922\n",
            "\n",
            "=== Outer Fold 3/4 ===\n",
            "[Fold 2] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.0882, Val Loss: 1.8442, Time: 8.27s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 0.4013, Val Loss: 0.4126, Time: 8.27s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.1582, Val Loss: 0.1577, Time: 8.27s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.1192, Val Loss: 0.1630, Time: 8.29s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.0689, Val Loss: 0.1210, Time: 8.28s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.0771, Val Loss: 0.0528, Time: 8.27s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.0434, Val Loss: 0.0360, Time: 8.27s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.0300, Val Loss: 0.0754, Time: 8.27s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.0281, Val Loss: 0.0190, Time: 8.27s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.0264, Val Loss: 0.0360, Time: 8.27s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.0161, Val Loss: 0.0252, Time: 8.28s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.1128, Val Loss: 0.3422, Time: 8.27s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.0127, Val Loss: 0.0288, Time: 8.29s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.0158, Val Loss: 0.0254, Time: 8.28s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.0092, Val Loss: 0.0154, Time: 8.31s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.0020, Val Loss: 0.0205, Time: 8.31s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.0023, Val Loss: 0.0067, Time: 8.31s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.0007, Val Loss: 0.0136, Time: 8.31s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.0004, Val Loss: 0.0160, Time: 8.29s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.0018, Val Loss: 0.0573, Time: 8.29s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.0065, Val Loss: 0.0136, Time: 8.27s\n",
            "  Result: Acc=0.9875, F1=0.9875\n",
            "\n",
            "=== Outer Fold 4/4 ===\n",
            "[Fold 3] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 0.9685, Val Loss: 1.4337, Time: 8.22s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 0.3882, Val Loss: 0.3721, Time: 8.23s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.1578, Val Loss: 0.1847, Time: 8.25s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.0597, Val Loss: 0.3644, Time: 8.27s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.0955, Val Loss: 0.2401, Time: 8.28s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.0226, Val Loss: 0.0588, Time: 8.28s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.0202, Val Loss: 0.0107, Time: 8.28s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.0154, Val Loss: 0.0350, Time: 8.28s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.0130, Val Loss: 0.0442, Time: 8.28s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.0023, Val Loss: 0.0071, Time: 8.29s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.0440, Val Loss: 0.4415, Time: 8.28s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.0065, Val Loss: 0.0146, Time: 8.28s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.0015, Val Loss: 0.0027, Time: 8.28s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.0020, Val Loss: 0.0015, Time: 8.27s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.0692, Val Loss: 0.0725, Time: 8.28s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.0034, Val Loss: 0.0045, Time: 8.29s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.0040, Val Loss: 0.0039, Time: 8.31s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.0030, Val Loss: 0.0242, Time: 8.32s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.0089, Val Loss: 0.0049, Time: 8.30s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.0050, Val Loss: 0.0237, Time: 8.29s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.0048, Val Loss: 0.0191, Time: 8.26s\n",
            "  Result: Acc=0.9437, F1=0.9442\n",
            "\n",
            "=== Final Results ===\n",
            "Mean Accuracy: 0.9179\n"
          ]
        }
      ],
      "source": [
        "# suppose dataset is already a DeepDataset like before\n",
        "input_length = deep_dataset_time[0]['signal'][0].shape[-1]\n",
        "num_classes = len(set([s['metainfo']['label'] for s in deep_dataset_time]))\n",
        "\n",
        "print(f\"Input length: {input_length}, Num classes: {num_classes}\")\n",
        "model = ResNet18_12k(input_length=input_length, num_classes=num_classes)\n",
        "\n",
        "exp = DeepLearningExperiment(\n",
        "    name=\"resnet18_12k_vibration\",\n",
        "    description=\"ResNet18 for Raw Vibration (12k points)\",\n",
        "    dataset=deep_dataset_time,\n",
        "    data_fold_idxs=folds_singleround_deep,  # numpy array of fold indexes\n",
        "    model=model,\n",
        "    batch_size=64,\n",
        "    lr=3e-4,\n",
        "    num_epochs=100\n",
        ")\n",
        "\n",
        "results = exp.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1D AlexNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AlexNet1D_12k(nn.Module):\n",
        "    def __init__(self, input_length=12000, in_channel=1, num_classes=10):\n",
        "        super(AlexNet1D_12k, self).__init__()\n",
        "        \n",
        "        self.features = nn.Sequential(\n",
        "            # Conv1: Kernel 11 e Stride 4.\n",
        "            # Reduz entrada 12000 -> ~3000\n",
        "            nn.Conv1d(in_channel, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool1d(kernel_size=3, stride=2), # 3000 -> 1500\n",
        "            \n",
        "            # Conv2\n",
        "            nn.Conv1d(64, 192, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool1d(kernel_size=3, stride=2), # 1500 -> 750\n",
        "            \n",
        "            # Conv3\n",
        "            nn.Conv1d(192, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            # Conv4\n",
        "            nn.Conv1d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            # Conv5\n",
        "            nn.Conv1d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool1d(kernel_size=3, stride=2), # 750 -> 375\n",
        "        )\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool1d(6)\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            # 256 canais * 6 dimensão temporal = 1536 features\n",
        "            nn.Linear(256 * 6, 1024), # Mantido 1024 conforme seu código (o paper original usa 4096)\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Tratamento de segurança para dimensão (Batch, 1, 12000)\n",
        "        if x.ndim == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "            \n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        \n",
        "        # Flatten robusto\n",
        "        x = torch.flatten(x, 1)\n",
        "        \n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input length: 12000, Num classes: 4\n",
            "\n",
            "=== Outer Fold 1/4 ===\n",
            "[Fold 0] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.3271, Val Loss: 1.3766, Time: 2.31s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 1.0680, Val Loss: 0.9905, Time: 2.30s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.7042, Val Loss: 0.6632, Time: 2.32s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.5989, Val Loss: 0.5328, Time: 2.22s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.4283, Val Loss: 0.3905, Time: 2.18s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.3922, Val Loss: 0.3205, Time: 2.15s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.3354, Val Loss: 0.2157, Time: 2.14s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.2160, Val Loss: 0.2193, Time: 2.16s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.1759, Val Loss: 0.1827, Time: 2.18s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.2581, Val Loss: 0.2227, Time: 2.21s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.1632, Val Loss: 0.1383, Time: 2.20s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.1509, Val Loss: 0.1382, Time: 2.19s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.1228, Val Loss: 0.1304, Time: 2.17s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.1113, Val Loss: 0.0897, Time: 2.16s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.1058, Val Loss: 0.1969, Time: 2.16s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.0705, Val Loss: 0.1307, Time: 2.16s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.0440, Val Loss: 0.0345, Time: 2.17s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.0389, Val Loss: 0.0347, Time: 2.16s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.1217, Val Loss: 0.0832, Time: 2.17s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.0331, Val Loss: 0.0338, Time: 2.16s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.1227, Val Loss: 0.0648, Time: 2.16s\n",
            "  Result: Acc=0.7517, F1=0.7473\n",
            "\n",
            "=== Outer Fold 2/4 ===\n",
            "[Fold 1] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.3206, Val Loss: 1.3051, Time: 2.14s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 1.0785, Val Loss: 0.9504, Time: 2.16s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.7529, Val Loss: 0.7116, Time: 2.14s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.5295, Val Loss: 0.5251, Time: 2.16s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.4134, Val Loss: 0.4542, Time: 2.15s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.3689, Val Loss: 0.2938, Time: 2.15s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.2662, Val Loss: 0.2688, Time: 2.14s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.2589, Val Loss: 0.2130, Time: 2.14s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.2377, Val Loss: 0.2279, Time: 2.14s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.1881, Val Loss: 0.1531, Time: 2.14s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.1873, Val Loss: 0.1883, Time: 2.14s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.1650, Val Loss: 0.1599, Time: 2.14s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.1641, Val Loss: 0.1930, Time: 2.14s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.1414, Val Loss: 0.1345, Time: 2.13s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.1556, Val Loss: 0.1302, Time: 2.13s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.1390, Val Loss: 0.1543, Time: 2.13s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.1489, Val Loss: 0.1182, Time: 2.13s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.1300, Val Loss: 0.1181, Time: 2.12s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.1126, Val Loss: 0.1153, Time: 2.13s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.1068, Val Loss: 0.1103, Time: 2.12s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.1514, Val Loss: 0.1797, Time: 2.12s\n",
            "  Result: Acc=0.8641, F1=0.8640\n",
            "\n",
            "=== Outer Fold 3/4 ===\n",
            "[Fold 2] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.3143, Val Loss: 1.2853, Time: 2.15s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 1.0845, Val Loss: 1.0353, Time: 2.15s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.7125, Val Loss: 0.6909, Time: 2.14s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.5827, Val Loss: 0.5979, Time: 2.14s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.4953, Val Loss: 0.5323, Time: 2.14s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.4810, Val Loss: 0.4650, Time: 2.13s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.3866, Val Loss: 0.4219, Time: 2.13s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.3317, Val Loss: 0.3154, Time: 2.13s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.2827, Val Loss: 0.2572, Time: 2.14s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.2630, Val Loss: 0.2479, Time: 2.13s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.2187, Val Loss: 0.2545, Time: 2.13s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.2111, Val Loss: 0.1917, Time: 2.14s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.2380, Val Loss: 0.2009, Time: 2.12s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.1777, Val Loss: 0.1772, Time: 2.12s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.2073, Val Loss: 0.1888, Time: 2.12s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.1886, Val Loss: 0.1637, Time: 2.13s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.1620, Val Loss: 0.1678, Time: 2.12s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.1483, Val Loss: 0.1264, Time: 2.13s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.1342, Val Loss: 0.1307, Time: 2.13s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.1185, Val Loss: 0.1488, Time: 2.12s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.1380, Val Loss: 0.1386, Time: 2.12s\n",
            "  Result: Acc=0.9297, F1=0.9294\n",
            "\n",
            "=== Outer Fold 4/4 ===\n",
            "[Fold 3] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.3199, Val Loss: 1.2841, Time: 2.17s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 1.0421, Val Loss: 0.9711, Time: 2.16s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.7474, Val Loss: 0.8243, Time: 2.15s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.5688, Val Loss: 0.6239, Time: 2.15s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.4287, Val Loss: 0.5255, Time: 2.14s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.3840, Val Loss: 0.7480, Time: 2.14s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.2847, Val Loss: 0.3466, Time: 2.14s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.2465, Val Loss: 0.3133, Time: 2.14s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.2431, Val Loss: 0.3279, Time: 2.13s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.1870, Val Loss: 0.2280, Time: 2.14s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.2056, Val Loss: 0.2191, Time: 2.13s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.2189, Val Loss: 0.2529, Time: 2.12s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.1591, Val Loss: 0.2688, Time: 2.13s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.1707, Val Loss: 0.1989, Time: 2.13s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.1666, Val Loss: 0.1960, Time: 2.13s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.1365, Val Loss: 0.1976, Time: 2.13s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.1508, Val Loss: 0.1773, Time: 2.12s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.1537, Val Loss: 0.1629, Time: 2.13s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.1178, Val Loss: 0.1450, Time: 2.12s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.1398, Val Loss: 0.4905, Time: 2.13s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.1363, Val Loss: 0.1837, Time: 2.13s\n",
            "  Result: Acc=0.8016, F1=0.8005\n",
            "\n",
            "=== Final Results ===\n",
            "Mean Accuracy: 0.8367\n"
          ]
        }
      ],
      "source": [
        "# suppose dataset is already a DeepDataset like before\n",
        "input_length = deep_dataset_time[0]['signal'][0].shape[-1]\n",
        "num_classes = len(set([s['metainfo']['label'] for s in deep_dataset_time]))\n",
        "\n",
        "print(f\"Input length: {input_length}, Num classes: {num_classes}\")\n",
        "model = AlexNet1D_12k(input_length=input_length, in_channel=1, num_classes=num_classes)\n",
        "\n",
        "exp = DeepLearningExperiment(\n",
        "    name=\"alexNet_12k_vibration\",\n",
        "    description=\"AlexNet for Raw Vibration (12k points)\",\n",
        "    dataset=deep_dataset_time,\n",
        "    data_fold_idxs=folds_singleround_deep,  # numpy array of fold indexes\n",
        "    model=model,\n",
        "    batch_size=64,\n",
        "    lr=3e-4,\n",
        "    num_epochs=100\n",
        ")\n",
        "\n",
        "results = exp.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results.show_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1D BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BiLSTM_12k(nn.Module):\n",
        "    def __init__(self, in_channel=1, out_channel=10):\n",
        "        super(BiLSTM_12k, self).__init__()\n",
        "        \n",
        "        # Hiperparâmetros Internos\n",
        "        self.hidden_dim = 64\n",
        "        self.kernel_num = 16\n",
        "        self.num_layers = 2\n",
        "        \n",
        "        # self.V define o comprimento da sequência que entra na LSTM\n",
        "        self.V = 100 \n",
        "\n",
        "        # Camada de stem\n",
        "        self.embed1 = nn.Sequential(\n",
        "            # Kernel grande e Stride 4 para lidar com alta taxa de amostragem\n",
        "            nn.Conv1d(in_channel, self.kernel_num, kernel_size=64, stride=4, padding=30),\n",
        "            nn.BatchNorm1d(self.kernel_num),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool1d(kernel_size=4, stride=4)\n",
        "        )\n",
        "        \n",
        "        self.embed2 = nn.Sequential(\n",
        "            nn.Conv1d(self.kernel_num, self.kernel_num*2, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(self.kernel_num*2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # AdaptiveMaxPool força a saída a ter exatamente comprimento self.V (100) garantindo entrada constante para a LSTM\n",
        "            nn.AdaptiveMaxPool1d(self.V)\n",
        "        )\n",
        "        \n",
        "        # Camada Recorrente (BiLSTM)\n",
        "        # Input Size: kernel_num*2 (32 features por passo de tempo)\n",
        "        self.bilstm = nn.LSTM(\n",
        "            input_size=self.kernel_num*2, \n",
        "            hidden_size=self.hidden_dim,\n",
        "            num_layers=self.num_layers, \n",
        "            bidirectional=True,\n",
        "            batch_first=True, \n",
        "            bias=False\n",
        "        )\n",
        "\n",
        "        # Classificador\n",
        "        # O input da linear é: Comprimento da Sequência (V) * (Hidden * 2 direções)\n",
        "        self.hidden2label1 = nn.Sequential(\n",
        "            nn.Linear(self.V * 2 * self.hidden_dim, 512),\n",
        "            nn.ReLU(), \n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        \n",
        "        self.hidden2label2 = nn.Linear(512, out_channel)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (Batch, 1, 12000)\n",
        "        if x.ndim == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "            \n",
        "        # Extração de Features Convolucionais\n",
        "        x = self.embed1(x)\n",
        "        x = self.embed2(x) \n",
        "        # (Batch, 32, 100) -> (Batch, Channels, Time)\n",
        "        \n",
        "        # Preparação para LSTM\n",
        "        # LSTM espera (Batch, Time, Features/Channels)\n",
        "        x = x.permute(0, 2, 1) # (Batch, 100, 32)\n",
        "        \n",
        "        # Processamento Recorrente\n",
        "        bilstm_out, _ = self.bilstm(x)\n",
        "        bilstm_out = torch.tanh(bilstm_out)\n",
        "        \n",
        "        # Classificação\n",
        "        bilstm_out = bilstm_out.reshape(bilstm_out.size(0), -1)\n",
        "        \n",
        "        logit = self.hidden2label1(bilstm_out)\n",
        "        logit = self.hidden2label2(logit)\n",
        "\n",
        "        return logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input length: 12000, Num classes: 4\n",
            "\n",
            "=== Outer Fold 1/4 ===\n",
            "[Fold 0] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.1788, Val Loss: 1.2263, Time: 0.69s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 0.6518, Val Loss: 0.5783, Time: 0.28s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.3748, Val Loss: 0.3169, Time: 0.28s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.3095, Val Loss: 0.2150, Time: 0.28s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.1914, Val Loss: 0.1472, Time: 0.29s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.1890, Val Loss: 0.1332, Time: 0.29s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.1080, Val Loss: 0.0673, Time: 0.35s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.0530, Val Loss: 0.0592, Time: 0.29s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.1046, Val Loss: 0.0905, Time: 0.28s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.0727, Val Loss: 0.1165, Time: 0.29s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.0476, Val Loss: 0.0353, Time: 0.29s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.0335, Val Loss: 0.0500, Time: 0.29s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.0968, Val Loss: 0.4930, Time: 0.29s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.0272, Val Loss: 0.0385, Time: 0.29s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.0595, Val Loss: 0.0675, Time: 0.34s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.0221, Val Loss: 0.0459, Time: 0.30s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.1240, Val Loss: 0.1128, Time: 0.29s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.0191, Val Loss: 0.0333, Time: 0.29s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.0397, Val Loss: 0.0305, Time: 0.29s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.0073, Val Loss: 0.0272, Time: 0.29s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.0142, Val Loss: 0.0289, Time: 0.30s\n",
            "  Result: Acc=0.7017, F1=0.6892\n",
            "\n",
            "=== Outer Fold 2/4 ===\n",
            "[Fold 1] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.1659, Val Loss: 1.1907, Time: 0.29s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 0.6591, Val Loss: 0.5897, Time: 0.29s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.5026, Val Loss: 0.4833, Time: 0.34s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.3396, Val Loss: 0.3195, Time: 0.28s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.2896, Val Loss: 0.2265, Time: 0.29s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.1998, Val Loss: 0.2596, Time: 0.29s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.1687, Val Loss: 0.1532, Time: 0.29s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.1221, Val Loss: 0.1239, Time: 0.29s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.1442, Val Loss: 0.1192, Time: 0.29s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.0725, Val Loss: 0.0876, Time: 0.28s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.0848, Val Loss: 0.0749, Time: 0.35s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.1370, Val Loss: 0.1235, Time: 0.28s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.0540, Val Loss: 0.0684, Time: 0.28s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.0687, Val Loss: 0.0956, Time: 0.29s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.1434, Val Loss: 0.1541, Time: 0.28s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.0474, Val Loss: 0.0674, Time: 0.28s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.0591, Val Loss: 0.0322, Time: 0.29s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.0304, Val Loss: 0.0499, Time: 0.28s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.0473, Val Loss: 0.1028, Time: 0.36s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.0320, Val Loss: 0.0305, Time: 0.28s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.0948, Val Loss: 0.0620, Time: 0.29s\n",
            "  Result: Acc=0.9234, F1=0.9222\n",
            "\n",
            "=== Outer Fold 3/4 ===\n",
            "[Fold 2] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.1704, Val Loss: 1.1379, Time: 0.28s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 0.6582, Val Loss: 0.5734, Time: 0.29s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.4834, Val Loss: 0.3923, Time: 0.28s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.3438, Val Loss: 0.3196, Time: 0.28s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.3029, Val Loss: 0.2413, Time: 0.29s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.2622, Val Loss: 0.2139, Time: 0.32s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.1618, Val Loss: 0.1492, Time: 0.34s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.1494, Val Loss: 0.1261, Time: 0.28s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.1715, Val Loss: 0.1538, Time: 0.28s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.1078, Val Loss: 0.1003, Time: 0.28s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.1388, Val Loss: 0.1296, Time: 0.28s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.1178, Val Loss: 0.0902, Time: 0.29s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.1726, Val Loss: 0.0947, Time: 0.28s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.0960, Val Loss: 0.0895, Time: 0.31s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.0372, Val Loss: 0.0524, Time: 0.34s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.0380, Val Loss: 0.0592, Time: 0.28s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.0819, Val Loss: 0.0848, Time: 0.28s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.0297, Val Loss: 0.0559, Time: 0.28s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.0462, Val Loss: 0.0592, Time: 0.29s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.0925, Val Loss: 0.1423, Time: 0.28s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.0475, Val Loss: 0.0467, Time: 0.28s\n",
            "  Result: Acc=0.9828, F1=0.9827\n",
            "\n",
            "=== Outer Fold 4/4 ===\n",
            "[Fold 3] Classifier training (100 epochs)...\n",
            "  [Supervised] Epoch 1/100 Train Loss: 1.1602, Val Loss: 1.1769, Time: 0.28s\n",
            "  [Supervised] Epoch 5/100 Train Loss: 0.6227, Val Loss: 0.5561, Time: 0.35s\n",
            "  [Supervised] Epoch 10/100 Train Loss: 0.4119, Val Loss: 0.2763, Time: 0.29s\n",
            "  [Supervised] Epoch 15/100 Train Loss: 0.3497, Val Loss: 0.2300, Time: 0.28s\n",
            "  [Supervised] Epoch 20/100 Train Loss: 0.3432, Val Loss: 0.2417, Time: 0.28s\n",
            "  [Supervised] Epoch 25/100 Train Loss: 0.2334, Val Loss: 0.1354, Time: 0.28s\n",
            "  [Supervised] Epoch 30/100 Train Loss: 0.1746, Val Loss: 0.0993, Time: 0.28s\n",
            "  [Supervised] Epoch 35/100 Train Loss: 0.0984, Val Loss: 0.0502, Time: 0.29s\n",
            "  [Supervised] Epoch 40/100 Train Loss: 0.1040, Val Loss: 0.0479, Time: 0.28s\n",
            "  [Supervised] Epoch 45/100 Train Loss: 0.1566, Val Loss: 0.0752, Time: 0.35s\n",
            "  [Supervised] Epoch 50/100 Train Loss: 0.0582, Val Loss: 0.0297, Time: 0.29s\n",
            "  [Supervised] Epoch 55/100 Train Loss: 0.0905, Val Loss: 0.0522, Time: 0.28s\n",
            "  [Supervised] Epoch 60/100 Train Loss: 0.0456, Val Loss: 0.0369, Time: 0.28s\n",
            "  [Supervised] Epoch 65/100 Train Loss: 0.0589, Val Loss: 0.0300, Time: 0.29s\n",
            "  [Supervised] Epoch 70/100 Train Loss: 0.0739, Val Loss: 0.1076, Time: 0.29s\n",
            "  [Supervised] Epoch 75/100 Train Loss: 0.0784, Val Loss: 0.0730, Time: 0.29s\n",
            "  [Supervised] Epoch 80/100 Train Loss: 0.0572, Val Loss: 0.0189, Time: 0.29s\n",
            "  [Supervised] Epoch 85/100 Train Loss: 0.0215, Val Loss: 0.0316, Time: 0.34s\n",
            "  [Supervised] Epoch 90/100 Train Loss: 0.0458, Val Loss: 0.0204, Time: 0.29s\n",
            "  [Supervised] Epoch 95/100 Train Loss: 0.0189, Val Loss: 0.0255, Time: 0.29s\n",
            "  [Supervised] Epoch 100/100 Train Loss: 0.0348, Val Loss: 0.0157, Time: 0.29s\n",
            "  Result: Acc=0.7781, F1=0.7732\n",
            "\n",
            "=== Final Results ===\n",
            "Mean Accuracy: 0.8465\n"
          ]
        }
      ],
      "source": [
        "# suppose dataset is already a DeepDataset like before\n",
        "input_length = deep_dataset_time[0]['signal'][0].shape[-1]\n",
        "num_classes = len(set([s['metainfo']['label'] for s in deep_dataset_time]))\n",
        "\n",
        "print(f\"Input length: {input_length}, Num classes: {num_classes}\")\n",
        "model = BiLSTM_12k(in_channel=1, out_channel=num_classes)\n",
        "\n",
        "exp = DeepLearningExperiment(\n",
        "    name=\"BiLSTM_12k_vibration\",\n",
        "    description=\"BiLSTM for Raw Vibration (12k points)\",\n",
        "    dataset=deep_dataset_time,\n",
        "    data_fold_idxs=folds_singleround_deep,  # numpy array of fold indexes\n",
        "    model=model,\n",
        "    batch_size=64,\n",
        "    lr=3e-4,\n",
        "    num_epochs=100\n",
        ")\n",
        "\n",
        "results = exp.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results.show_results()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "PPXmkt-iKCeA",
        "Bt4WUe1EkNUQ",
        "pEE_ePYwV08q",
        "9RzAtB1FtI1t",
        "5y2zAwCJuAkM"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
